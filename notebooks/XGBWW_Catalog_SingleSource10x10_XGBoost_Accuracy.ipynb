{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/CalculatedContent/xgboost2ww/blob/main/notebooks/XGBWW_Catalog_SingleSource10x10_XGBoost_Accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBWW catalog single-source 10x10 XGBoost benchmark\n",
    "\n",
    "This notebook:\n",
    "1. Loads the dataset catalog checkpoint.\n",
    "2. Selects **one source** and samples **10 random classification datasets** from that source.\n",
    "3. Trains **10 XGBoost hyperparameter settings per dataset** (100 total runs) using a hold-out set.\n",
    "4. Runs `xgboost2ww` + `WeightWatcher.analyze(ERG=True, randomize=True)` on each trained model.\n",
    "5. Tracks `alpha`, `ERG_gap`, and `rand_distance` against test accuracy.\n",
    "6. Produces scatter plots per model and then a combined multi-model view.\n",
    "7. Saves all outputs to an experiment checkpoint directory on Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Mount Google Drive and configure experiment paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ===== USER CONFIG =====\n",
    "CATALOG_CSV = Path('/content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv')\n",
    "EXPERIMENT_ROOT = Path('/content/drive/MyDrive/xgbwwdata/experiment_checkpoints')\n",
    "EXPERIMENT_NAME = 'single_source_10models_10hp_xgboost_accuracy'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.20\n",
    "TARGET_SOURCE = None  # Example: 'openml'. If None, auto-pick a source with >= MODELS_PER_SOURCE datasets.\n",
    "MODELS_PER_SOURCE = 10\n",
    "HP_SETTINGS_PER_MODEL = 10\n",
    "# =======================\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "EXPERIMENT_ID = f\"{EXPERIMENT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "CHECKPOINT_DIR = EXPERIMENT_ROOT / EXPERIMENT_ID\n",
    "PLOTS_DIR = CHECKPOINT_DIR / 'plots'\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Catalog path:', CATALOG_CSV)\n",
    "print('Checkpoint directory:', CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/repo_xgbwwdata\n",
    "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\n",
    "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\n",
    "\n",
    "%pip install -q openml pmlb keel-ds xgboost scikit-learn xgboost2ww weightwatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import weightwatcher as ww\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgbwwdata import Filters, load_dataset\n",
    "from xgboost2ww import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Load catalog and sample 10 random datasets from one source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CATALOG_CSV.exists():\n",
    "    raise FileNotFoundError(f'Catalog not found: {CATALOG_CSV}. Run dataset catalog checkpoint notebook first.')\n",
    "\n",
    "df_catalog = pd.read_csv(CATALOG_CSV)\n",
    "print('Catalog shape:', df_catalog.shape)\n",
    "\n",
    "required_cols = {'dataset_uid', 'source', 'task_type'}\n",
    "missing = required_cols - set(df_catalog.columns)\n",
    "if missing:\n",
    "    raise ValueError(f'Catalog is missing required columns: {missing}')\n",
    "\n",
    "df_cls = df_catalog[df_catalog['task_type'].astype(str).str.contains('classification', case=False, na=False)].copy()\n",
    "if df_cls.empty:\n",
    "    raise ValueError('No classification datasets found in catalog.')\n",
    "\n",
    "source_counts = df_cls.groupby('source')['dataset_uid'].nunique().sort_values(ascending=False)\n",
    "display(source_counts.to_frame('n_classification_datasets'))\n",
    "\n",
    "if TARGET_SOURCE is None:\n",
    "    eligible = source_counts[source_counts >= MODELS_PER_SOURCE]\n",
    "    if eligible.empty:\n",
    "        raise ValueError(f'No source has at least {MODELS_PER_SOURCE} classification datasets.')\n",
    "    chosen_source = eligible.index[0]\n",
    "else:\n",
    "    chosen_source = TARGET_SOURCE\n",
    "\n",
    "source_pool = df_cls[df_cls['source'] == chosen_source].drop_duplicates('dataset_uid').copy()\n",
    "if len(source_pool) < MODELS_PER_SOURCE:\n",
    "    raise ValueError(f\"Source '{chosen_source}' has only {len(source_pool)} datasets; need {MODELS_PER_SOURCE}.\")\n",
    "\n",
    "df_pick = source_pool.sample(n=MODELS_PER_SOURCE, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "print(f\"Chosen source: {chosen_source}\")\n",
    "print(f\"Selected {len(df_pick)} datasets from source='{chosen_source}'\")\n",
    "display(df_pick[['source', 'dataset_uid', 'name', 'task_type']].sort_values('dataset_uid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Define robust hyperparameter settings (10 per model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced settings designed to avoid severe overfitting while still covering a useful range.\n",
    "hyperparameter_grid = [\n",
    "    dict(learning_rate=0.03, max_depth=3, min_child_weight=3.0, subsample=0.80, colsample_bytree=0.80, reg_lambda=3.0, reg_alpha=0.0),\n",
    "    dict(learning_rate=0.05, max_depth=4, min_child_weight=2.0, subsample=0.85, colsample_bytree=0.85, reg_lambda=2.0, reg_alpha=0.0),\n",
    "    dict(learning_rate=0.07, max_depth=4, min_child_weight=3.0, subsample=0.90, colsample_bytree=0.90, reg_lambda=2.0, reg_alpha=0.0),\n",
    "    dict(learning_rate=0.05, max_depth=5, min_child_weight=4.0, subsample=0.80, colsample_bytree=0.85, reg_lambda=4.0, reg_alpha=0.05),\n",
    "    dict(learning_rate=0.08, max_depth=5, min_child_weight=3.0, subsample=0.85, colsample_bytree=0.80, reg_lambda=3.0, reg_alpha=0.10),\n",
    "    dict(learning_rate=0.04, max_depth=6, min_child_weight=5.0, subsample=0.75, colsample_bytree=0.75, reg_lambda=5.0, reg_alpha=0.10),\n",
    "    dict(learning_rate=0.06, max_depth=6, min_child_weight=4.0, subsample=0.80, colsample_bytree=0.80, reg_lambda=4.0, reg_alpha=0.15),\n",
    "    dict(learning_rate=0.10, max_depth=4, min_child_weight=2.0, subsample=0.90, colsample_bytree=0.90, reg_lambda=1.5, reg_alpha=0.00),\n",
    "    dict(learning_rate=0.03, max_depth=7, min_child_weight=6.0, subsample=0.70, colsample_bytree=0.70, reg_lambda=6.0, reg_alpha=0.20),\n",
    "    dict(learning_rate=0.09, max_depth=5, min_child_weight=2.0, subsample=0.85, colsample_bytree=0.85, reg_lambda=2.5, reg_alpha=0.05),\n",
    "]\n",
    "\n",
    "assert len(hyperparameter_grid) == HP_SETTINGS_PER_MODEL\n",
    "pd.DataFrame(hyperparameter_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Train 10x10 models, evaluate hold-out accuracy, and compute WW metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = Filters(min_rows=200, max_rows=60000, max_features=50000, max_dense_elements=int(2e8))\n",
    "\n",
    "\n",
    "def pick_metric(details_df: pd.DataFrame, candidates):\n",
    "    for c in candidates:\n",
    "        if c in details_df.columns and len(details_df[c].dropna()) > 0:\n",
    "            return float(details_df[c].dropna().iloc[0])\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def train_eval_one_setting(X_train, X_test, y_train, y_test, n_classes, hp, seed):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'tree_method': 'hist',\n",
    "            'seed': seed,\n",
    "            **hp,\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'objective': 'multi:softprob',\n",
    "            'num_class': n_classes,\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'tree_method': 'hist',\n",
    "            'seed': seed,\n",
    "            **hp,\n",
    "        }\n",
    "\n",
    "    evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "    bst = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1200,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=40,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    if n_classes == 2:\n",
    "        train_prob = bst.predict(dtrain)\n",
    "        test_prob = bst.predict(dtest)\n",
    "        yhat_train = (train_prob >= 0.5).astype(int)\n",
    "        yhat_test = (test_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        train_prob = bst.predict(dtrain)\n",
    "        test_prob = bst.predict(dtest)\n",
    "        yhat_train = np.argmax(train_prob, axis=1)\n",
    "        yhat_test = np.argmax(test_prob, axis=1)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, yhat_train)\n",
    "    test_acc = accuracy_score(y_test, yhat_test)\n",
    "\n",
    "    layer = convert(\n",
    "        bst,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        W='W7',\n",
    "        return_type='torch',\n",
    "        nfolds=5,\n",
    "        t_points=40,\n",
    "        random_state=seed,\n",
    "        train_params=params,\n",
    "        num_boost_round=int(bst.best_iteration + 1),\n",
    "    )\n",
    "\n",
    "    watcher = ww.WeightWatcher(model=layer)\n",
    "    details_df = watcher.analyze(ERG=True, randomize=True, plot=False)\n",
    "\n",
    "    alpha = pick_metric(details_df, ['alpha'])\n",
    "    erg_gap = pick_metric(details_df, ['ERG_gap'])\n",
    "    rand_distance = pick_metric(details_df, ['rand_distance', 'random_distance', 'rand_dist'])\n",
    "\n",
    "    return {\n",
    "        'train_accuracy': float(train_acc),\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'overfit_gap': float(train_acc - test_acc),\n",
    "        'best_iteration': int(bst.best_iteration + 1),\n",
    "        'alpha': alpha,\n",
    "        'ERG_gap': erg_gap,\n",
    "        'rand_distance': rand_distance,\n",
    "    }\n",
    "\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "for model_idx, row in df_pick.reset_index(drop=True).iterrows():\n",
    "    dataset_uid = row['dataset_uid']\n",
    "    source = row['source']\n",
    "\n",
    "    try:\n",
    "        X, y, _meta = load_dataset(dataset_uid, filters=filters)\n",
    "        y = np.asarray(y)\n",
    "        classes, y_enc = np.unique(y, return_inverse=True)\n",
    "        n_classes = len(classes)\n",
    "        if n_classes < 2:\n",
    "            raise ValueError('dataset has <2 classes')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_enc,\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=y_enc,\n",
    "        )\n",
    "\n",
    "        for hp_idx, hp in enumerate(hyperparameter_grid):\n",
    "            run_seed = RANDOM_SEED + model_idx * 100 + hp_idx\n",
    "            try:\n",
    "                out = train_eval_one_setting(X_train, X_test, y_train, y_test, n_classes, hp, run_seed)\n",
    "                results.append({\n",
    "                    'source': source,\n",
    "                    'dataset_uid': dataset_uid,\n",
    "                    'dataset_name': row.get('name', str(dataset_uid)),\n",
    "                    'model_index': int(model_idx),\n",
    "                    'hp_index': int(hp_idx),\n",
    "                    'n_classes': int(n_classes),\n",
    "                    'n_train': int(X_train.shape[0]),\n",
    "                    'n_test': int(X_test.shape[0]),\n",
    "                    'n_features': int(X_train.shape[1]),\n",
    "                    **out,\n",
    "                    'params_json': json.dumps(hp, sort_keys=True),\n",
    "                })\n",
    "                print(f\"[OK] dataset={dataset_uid} hp={hp_idx} test_acc={out['test_accuracy']:.4f} gap={out['overfit_gap']:.4f}\")\n",
    "            except Exception as e:\n",
    "                errors.append({'dataset_uid': dataset_uid, 'hp_index': hp_idx, 'error': str(e)})\n",
    "                print(f\"[ERR] dataset={dataset_uid} hp={hp_idx}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        errors.append({'dataset_uid': dataset_uid, 'hp_index': None, 'error': str(e)})\n",
    "        print(f\"[SKIP] dataset={dataset_uid}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "errors_df = pd.DataFrame(errors)\n",
    "\n",
    "print('Successful runs:', len(results_df))\n",
    "print('Failed runs:', len(errors_df))\n",
    "\n",
    "if not results_df.empty:\n",
    "    display(results_df.head())\n",
    "if not errors_df.empty:\n",
    "    display(errors_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Select best hyperparameter setting per model and persist checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df.empty:\n",
    "    print('No successful runs to summarize.')\n",
    "else:\n",
    "    best_per_model = (\n",
    "        results_df.sort_values(['model_index', 'test_accuracy', 'overfit_gap'], ascending=[True, False, True])\n",
    "        .groupby('model_index', as_index=False)\n",
    "        .first()\n",
    "        .sort_values('model_index')\n",
    "    )\n",
    "\n",
    "    print('Best hyperparameter setting for each model (based on hold-out test accuracy):')\n",
    "    display(best_per_model[[\n",
    "        'model_index', 'dataset_uid', 'dataset_name', 'hp_index',\n",
    "        'train_accuracy', 'test_accuracy', 'overfit_gap',\n",
    "        'alpha', 'ERG_gap', 'rand_distance'\n",
    "    ]])\n",
    "\n",
    "    print('Overall summary:')\n",
    "    display(results_df[['train_accuracy','test_accuracy','overfit_gap','alpha','ERG_gap','rand_distance']].describe())\n",
    "\n",
    "    experiment_config = {\n",
    "        'experiment_id': EXPERIMENT_ID,\n",
    "        'experiment_name': EXPERIMENT_NAME,\n",
    "        'catalog_csv': str(CATALOG_CSV),\n",
    "        'chosen_source': chosen_source,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'test_size': TEST_SIZE,\n",
    "        'models_per_source': MODELS_PER_SOURCE,\n",
    "        'hp_settings_per_model': HP_SETTINGS_PER_MODEL,\n",
    "        'successful_runs': int(len(results_df)),\n",
    "        'failed_runs': int(len(errors_df)),\n",
    "    }\n",
    "\n",
    "    results_path = CHECKPOINT_DIR / 'results_all_runs.csv'\n",
    "    best_path = CHECKPOINT_DIR / 'best_per_model.csv'\n",
    "    errors_path = CHECKPOINT_DIR / 'errors.csv'\n",
    "    config_path = CHECKPOINT_DIR / 'experiment_config.json'\n",
    "\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    best_per_model.to_csv(best_path, index=False)\n",
    "    errors_df.to_csv(errors_path, index=False)\n",
    "    config_path.write_text(json.dumps(experiment_config, indent=2))\n",
    "\n",
    "    print('Saved:')\n",
    "    print('-', results_path)\n",
    "    print('-', best_path)\n",
    "    print('-', errors_path)\n",
    "    print('-', config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Scatter plots per model: WW metrics vs hold-out test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df.empty:\n",
    "    print('No successful runs to plot.')\n",
    "else:\n",
    "    metrics = ['alpha', 'ERG_gap', 'rand_distance']\n",
    "\n",
    "    for model_idx, g in results_df.groupby('model_index'):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 4.8), squeeze=False)\n",
    "        g = g.sort_values('hp_index')\n",
    "\n",
    "        for ax, m in zip(axes[0], metrics):\n",
    "            ax.scatter(g[m], g['test_accuracy'], s=65, alpha=0.85)\n",
    "            for _, r in g.iterrows():\n",
    "                ax.annotate(f\"hp{int(r['hp_index'])}\", (r[m], r['test_accuracy']), fontsize=8, alpha=0.8)\n",
    "            ax.set_xlabel(m)\n",
    "            ax.set_ylabel('Hold-out test accuracy')\n",
    "            ax.set_title(f'Model {model_idx} ({g.iloc[0][\"dataset_uid\"]})')\n",
    "            ax.grid(alpha=0.2)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        out_path = PLOTS_DIR / f'model_{int(model_idx):02d}_scatter.png'\n",
    "        fig.savefig(out_path, dpi=160, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    print('Saved per-model scatter plots to:', PLOTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Combined plot across all models and all hyperparameter runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_df.empty:\n",
    "    print('No successful runs to plot.')\n",
    "else:\n",
    "    metrics = ['alpha', 'ERG_gap', 'rand_distance']\n",
    "    n_models = results_df['model_index'].nunique()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(22, 14), squeeze=False)\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    for i, (model_idx, g) in enumerate(sorted(results_df.groupby('model_index'), key=lambda x: x[0])):\n",
    "        ax = axes_flat[i]\n",
    "        ax.scatter(g['alpha'], g['test_accuracy'], label='alpha', alpha=0.8, s=25)\n",
    "        ax.scatter(g['ERG_gap'], g['test_accuracy'], label='ERG_gap', alpha=0.8, s=25)\n",
    "        ax.scatter(g['rand_distance'], g['test_accuracy'], label='rand_distance', alpha=0.8, s=25)\n",
    "        ax.set_title(f\"Model {model_idx}: {g.iloc[0]['dataset_uid']}\")\n",
    "        ax.set_xlabel('Metric value')\n",
    "        ax.set_ylabel('Hold-out test acc')\n",
    "        ax.grid(alpha=0.2)\n",
    "\n",
    "    for j in range(i + 1, len(axes_flat)):\n",
    "        axes_flat[j].axis('off')\n",
    "\n",
    "    handles, labels = axes_flat[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=3)\n",
    "    fig.suptitle('Combined WW-metric trends vs hold-out test accuracy (all models)', y=0.98)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    combined_path = PLOTS_DIR / 'combined_all_models_scatter.png'\n",
    "    fig.savefig(combined_path, dpi=170, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    print('Saved combined plot:', combined_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}