{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalculatedContent/xgboost2ww/blob/main/GoodModelsXGBoost2WW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# xgboost2ww Experiment (100 models)\n",
        "\n",
        "This starter notebook trains 100 random XGBoost models and evaluates the result.  \n",
        "\n",
        "It shows you how to:\n",
        "1. Pick a “good” XGBoost model using **training-only cross-validation**.\n",
        "2. Evaluate it on a **true holdout test set** (never used in CV or OOF).\n",
        "3. Use **xgboost2ww.convert()** to build tiny PyTorch layers for the **W7** (default, recommended matrix)\n",
        "4. Run **WeightWatcher** to estimate:\n",
        "   - **α (alpha)**: heavy-tail exponent estimate\n",
        "   - **traps**: randomization spikes proxy (WeightWatcher diagnostic)\n",
        "\n",
        "**Note:** For an initial evaluation, you do **not** need `detX=True` in WeightWatcher."
      ],
      "metadata": {
        "id": "r79T_pncTmsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"pandas==2.2.2\" xgboost weightwatcher scikit-learn openml scipy pyarrow\n",
        "!apt-get -qq update && apt-get -qq install -y git\n",
        "\n",
        "# Clone + install xgboost2ww (public repo)\n",
        "!rm -rf /content/xgboost2ww\n",
        "!git clone https://github.com/CalculatedContent/xgboost2ww.git /content/xgboost2ww\n",
        "!pip -q install -e /content/xgboost2ww\n",
        "\n",
        "# Fix namespace-cache issues in Colab (safe to run always)\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/xgboost2ww/src\")\n",
        "sys.modules.pop(\"xgboost2ww\", None)\n",
        "\n",
        "import xgboost2ww\n",
        "print(\"xgboost2ww loaded from:\", getattr(xgboost2ww, \"__file__\", None))\n",
        "\n",
        "from xgboost2ww import convert\n",
        "print(\"Imported xgboost2ww.convert OK\")"
      ],
      "metadata": {
        "id": "uxFwluf9ToUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and settings\n",
        "\n",
        "We’ll run a small starter experiment over a handful of OpenML binary datasets.\n",
        "\n",
        "Key settings:\n",
        "- `TARGET_DATASETS`: how many datasets to run (starter: 10)\n",
        "- `NFOLDS`: OOF folds for xgboost2ww matrices\n",
        "- `T_TRAJ`: how many trajectory points along boosting to sample"
      ],
      "metadata": {
        "id": "LrWB074kTonQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings, time\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import openml\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import torch\n",
        "import weightwatcher as ww\n",
        "\n",
        "# Reproducibility\n",
        "RNG = 0\n",
        "rng = np.random.default_rng(RNG)\n",
        "\n",
        "# Starter run size\n",
        "TARGET_DATASETS = 50\n",
        "\n",
        "# Train/test split (true holdout)\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "# OOF matrix construction\n",
        "NFOLDS = 5\n",
        "T_TRAJ = 160\n",
        "\n",
        "# Data cap + safety guard\n",
        "MAX_OPENML_ROWS = 60000\n",
        "MAX_FEATURES_GUARD = 50_000\n",
        "\n",
        "# “Good model” selection (training-only CV)\n",
        "GOOD_TRIALS = 5\n",
        "CV_MAX_ROUNDS = 3000\n",
        "CV_EARLY_STOP = 150\n",
        "MIN_GOOD_TEST_ACC = 0.75\n",
        "\n",
        "# OpenML suites to pull datasets from\n",
        "SUITE_IDS = [14, 99, 225]"
      ],
      "metadata": {
        "id": "VaeRDLyaTovg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: GPU detection for XGBoost\n",
        "\n",
        "If a GPU is available, we’ll use XGBoost’s `gpu_hist` for faster training.\n",
        "If not, we fall back to CPU `hist`."
      ],
      "metadata": {
        "id": "fa_CgTaYTo3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xgb_gpu_available() -> bool:\n",
        "    try:\n",
        "        Xtmp = np.random.randn(256, 8).astype(np.float32)\n",
        "        ytmp = (Xtmp[:, 0] > 0).astype(np.int32)\n",
        "        dtmp = xgb.DMatrix(Xtmp, label=ytmp)\n",
        "        params = dict(\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"logloss\",\n",
        "            tree_method=\"gpu_hist\",\n",
        "            predictor=\"gpu_predictor\",\n",
        "            max_depth=2,\n",
        "            learning_rate=0.2,\n",
        "            seed=RNG,\n",
        "        )\n",
        "        _ = xgb.train(params=params, dtrain=dtmp, num_boost_round=5, verbose_eval=False)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "USE_GPU = xgb_gpu_available()\n",
        "print(\"XGBoost GPU available:\", USE_GPU)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-34rS7OATpAB",
        "outputId": "b6569eaa-ccf1-4e3e-d9ba-125287811b11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost GPU available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "## Load OpenML binary datasets (with preprocessing)\n",
        "\n",
        "We:\n",
        "- Keep only **binary** classification datasets\n",
        "- One-hot encode categorical features\n",
        "- Impute missing values\n",
        "- Optionally cap dataset size (`MAX_OPENML_ROWS`) for speed\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Qnm_xfUZTzwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def factorize_binary(y_raw):\n",
        "    y_codes, uniques = pd.factorize(y_raw)\n",
        "    if len(uniques) != 2:\n",
        "        return None\n",
        "    return y_codes.astype(int)\n",
        "\n",
        "def make_preprocessor(Xdf: pd.DataFrame):\n",
        "    cat_cols = Xdf.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "    num_cols = [c for c in Xdf.columns if c not in cat_cols]\n",
        "    transformers = []\n",
        "    if len(num_cols):\n",
        "        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols))\n",
        "    if len(cat_cols):\n",
        "        transformers.append((\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"oh\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
        "        ]), cat_cols))\n",
        "    if not transformers:\n",
        "        raise ValueError(\"no usable columns\")\n",
        "    return ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=0.3)\n",
        "\n",
        "def enumerate_openml_dataset_ids_from_suites(suite_ids):\n",
        "    ids, seen = [], set()\n",
        "    for sid in suite_ids:\n",
        "        suite = openml.study.get_suite(sid)\n",
        "        for did in suite.data:\n",
        "            did = int(did)\n",
        "            if did not in seen:\n",
        "                ids.append(did)\n",
        "                seen.add(did)\n",
        "    return ids\n",
        "\n",
        "def load_openml_dataset_by_id(did: int):\n",
        "    ds = openml.datasets.get_dataset(did)\n",
        "    target = ds.default_target_attribute\n",
        "    Xdf, y_raw, _, _ = ds.get_data(dataset_format=\"dataframe\", target=target)\n",
        "\n",
        "    y = factorize_binary(y_raw)\n",
        "    if y is None:\n",
        "        return None\n",
        "\n",
        "    if MAX_OPENML_ROWS is not None and len(Xdf) > MAX_OPENML_ROWS:\n",
        "        take = rng.choice(len(Xdf), size=MAX_OPENML_ROWS, replace=False)\n",
        "        Xdf = Xdf.iloc[take].reset_index(drop=True)\n",
        "        y = y[take]\n",
        "\n",
        "    pre = make_preprocessor(Xdf)\n",
        "    X = pre.fit_transform(Xdf)\n",
        "    return X, y.astype(int), ds.name, did"
      ],
      "metadata": {
        "id": "xLaWw5d_Tz3P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pick a “good” XGBoost model using training-only CV\n",
        "\n",
        "For each dataset:\n",
        "1. Split into **train/test**\n",
        "2. Run CV on the **train only** set to pick hyperparameters + early-stopping rounds\n",
        "3. Train a final model on train\n",
        "4. Evaluate once on the holdout test set\n",
        "\n",
        "We keep only datasets where the holdout test accuracy is at least `MIN_GOOD_TEST_ACC`."
      ],
      "metadata": {
        "id": "XcOM6Ow-Tz_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_good_params_via_cv(Xtr, ytr, nfold=5, *, dataset_id: int):\n",
        "    dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
        "    local_rng = np.random.default_rng(RNG + int(dataset_id))  # stable across runs\n",
        "\n",
        "    best = None\n",
        "    best_score = np.inf\n",
        "\n",
        "    for _ in range(GOOD_TRIALS):\n",
        "        params = dict(\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"logloss\",\n",
        "            tree_method=\"hist\",\n",
        "            seed=RNG,\n",
        "            learning_rate=float(10 ** local_rng.uniform(-2.0, -0.6)),   # 0.01..0.25\n",
        "            max_depth=int(local_rng.integers(2, 7)),\n",
        "            min_child_weight=float(10 ** local_rng.uniform(0.0, 2.0)),  # 1..100\n",
        "            subsample=float(local_rng.uniform(0.6, 0.9)),\n",
        "            colsample_bytree=float(local_rng.uniform(0.6, 0.9)),\n",
        "            reg_lambda=float(10 ** local_rng.uniform(0.0, 2.0)),        # 1..100\n",
        "            gamma=float(local_rng.uniform(0.0, 0.5)),\n",
        "        )\n",
        "        if USE_GPU:\n",
        "            params[\"tree_method\"] = \"gpu_hist\"\n",
        "            params[\"predictor\"] = \"gpu_predictor\"\n",
        "\n",
        "        cv = xgb.cv(\n",
        "            params=params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=CV_MAX_ROUNDS,\n",
        "            nfold=nfold,\n",
        "            stratified=True,\n",
        "            early_stopping_rounds=CV_EARLY_STOP,\n",
        "            seed=RNG,\n",
        "            verbose_eval=False,\n",
        "        )\n",
        "\n",
        "        score = float(cv[\"test-logloss-mean\"].iloc[-1])\n",
        "        rounds = int(len(cv))\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best = (params, rounds, score)\n",
        "\n",
        "    return best  # (params, rounds, cv_logloss)\n",
        "\n",
        "def train_eval_fulltrain(Xtr, ytr, Xte, yte, params, rounds):\n",
        "    dtr = xgb.DMatrix(Xtr, label=ytr)\n",
        "    dte = xgb.DMatrix(Xte, label=yte)\n",
        "\n",
        "    bst = xgb.train(params=params, dtrain=dtr, num_boost_round=rounds, verbose_eval=False)\n",
        "\n",
        "    m_tr = bst.predict(dtr, output_margin=True).astype(np.float32)\n",
        "    p_tr = 1.0 / (1.0 + np.exp(-m_tr))\n",
        "    train_acc = float(accuracy_score(ytr, (p_tr >= 0.5).astype(int)))\n",
        "\n",
        "    m_te = bst.predict(dte, output_margin=True).astype(np.float32)\n",
        "    p_te = 1.0 / (1.0 + np.exp(-m_te))\n",
        "    test_acc = float(accuracy_score(yte, (p_te >= 0.5).astype(int)))\n",
        "    test_loss = float(log_loss(yte, np.vstack([1 - p_te, p_te]).T, labels=[0, 1]))\n",
        "\n",
        "    return train_acc, test_acc, test_loss, bst"
      ],
      "metadata": {
        "id": "qxYlKLUpT0H9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WeightWatcher helper\n",
        "\n",
        "We use WeightWatcher on the PyTorch layer returned by `xgboost2ww.convert()`.\n",
        "\n",
        "For a first pass, we run:\n",
        "\n",
        "`watcher.analyze(randomize=True, plot=False)`\n",
        "\n",
        "No `detX=True` needed for initial evaluation."
      ],
      "metadata": {
        "id": "D2Vj0TwCT0Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ww_alpha_traps_from_layer(layer):\n",
        "    watcher = ww.WeightWatcher(model=layer)\n",
        "    details_df = watcher.analyze(randomize=True, plot=False)  # starter: no detX\n",
        "    alpha = float(details_df[\"alpha\"].iloc[0]) if \"alpha\" in details_df.columns else np.nan\n",
        "    traps = float(details_df[\"rand_num_spikes\"].iloc[0]) if \"rand_num_spikes\" in details_df.columns else np.nan\n",
        "    return alpha, traps"
      ],
      "metadata": {
        "id": "snaQwsuwT0Xy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the experiment (first 10 datasets)\n",
        "\n",
        "For each dataset, we compute α/traps for **W7**.\n",
        "\n",
        "Key reproducibility detail:\n",
        "- We pass `train_params=good_params` and `num_boost_round=good_rounds` into `convert()`\n",
        "- That ensures the fold-training used to compute OOF increments matches the chosen model configuration."
      ],
      "metadata": {
        "id": "-sSz-5yaUCzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MAIN LOOP — Compute W7 ONLY using xgboost2ww.convert()\n",
        "# Memory-safe version:\n",
        "#   - keep sparse when possible (XGBoost supports CSR)\n",
        "#   - avoid densifying big one-hot matrices\n",
        "#   - use float32\n",
        "#   - hard guards + cleanup\n",
        "# ============================================================\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "rows = []\n",
        "kept = 0\n",
        "\n",
        "# If you ever densify, this is the real RAM killer.\n",
        "# 2e8 float32 ~ 0.8GB. float64 would be ~1.6GB.\n",
        "MAX_DENSE_ELEMENTS = int(2e8)\n",
        "\n",
        "dataset_ids = enumerate_openml_dataset_ids_from_suites(SUITE_IDS)\n",
        "t0 = time.time()\n",
        "\n",
        "for did in dataset_ids:\n",
        "    if kept >= TARGET_DATASETS:\n",
        "        break\n",
        "\n",
        "    loaded = load_openml_dataset_by_id(int(did))\n",
        "    if loaded is None:\n",
        "        continue\n",
        "\n",
        "    X, y, name, did_loaded = loaded\n",
        "\n",
        "    # ---- Feature guard (still useful, but not sufficient alone)\n",
        "    if int(X.shape[1]) > MAX_FEATURES_GUARD:\n",
        "        continue\n",
        "\n",
        "    # -----------------------------\n",
        "    # True holdout split\n",
        "    # -----------------------------\n",
        "    tr_idx, te_idx = train_test_split(\n",
        "        np.arange(len(y)),\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=RNG,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    Xtr = X[tr_idx]\n",
        "    Xte = X[te_idx]\n",
        "    ytr = y[tr_idx]\n",
        "    yte = y[te_idx]\n",
        "\n",
        "    # -----------------------------\n",
        "    # Keep sparse if sparse; cast to float32 if dense\n",
        "    # -----------------------------\n",
        "    is_sparse = hasattr(Xtr, \"tocoo\")  # works for scipy sparse matrices\n",
        "\n",
        "    if not is_sparse:\n",
        "        # Dense path: force 2D arrays and float32\n",
        "        Xtr = np.asarray(Xtr, dtype=np.float32)\n",
        "        Xte = np.asarray(Xte, dtype=np.float32)\n",
        "\n",
        "        if Xtr.ndim != 2:\n",
        "            print(\"SKIP: unexpected Xtr shape:\", getattr(Xtr, \"shape\", None))\n",
        "            continue\n",
        "    else:\n",
        "        # Sparse path: DO NOT densify for CV/training\n",
        "        # Convert to CSR once (fast row slicing + DMatrix friendly)\n",
        "        Xtr = Xtr.tocsr()\n",
        "        Xte = Xte.tocsr()\n",
        "\n",
        "        # OPTIONAL: you may also enforce float32 data in CSR\n",
        "        # (scipy sparse supports astype efficiently)\n",
        "        try:\n",
        "            Xtr = Xtr.astype(np.float32)\n",
        "            Xte = Xte.astype(np.float32)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # HARD guard: if someone later tries to densify, estimate cost\n",
        "        dense_cost = int(Xtr.shape[0]) * int(Xtr.shape[1])\n",
        "        if dense_cost > MAX_DENSE_ELEMENTS:\n",
        "            # We can still train XGBoost on sparse,\n",
        "            # but convert() might require dense -> likely OOM.\n",
        "            # Safer to skip early unless you have a sparse-aware convert().\n",
        "            print(\n",
        "                f\"SKIP: sparse would densify too big: \"\n",
        "                f\"n_train={Xtr.shape[0]}, d={Xtr.shape[1]}, \"\n",
        "                f\"elements={dense_cost:,}\"\n",
        "            )\n",
        "            # cleanup\n",
        "            del X, y, Xtr, Xte, ytr, yte\n",
        "            gc.collect()\n",
        "            continue\n",
        "\n",
        "    # -----------------------------\n",
        "    # Select \"good\" hyperparameters (train-only CV)\n",
        "    # -----------------------------\n",
        "    good_params, good_rounds, good_cv_logloss = pick_good_params_via_cv(\n",
        "        Xtr, ytr, nfold=NFOLDS, dataset_id=int(did_loaded)\n",
        "    )\n",
        "\n",
        "    # Train final model and evaluate on holdout\n",
        "    good_train_acc, good_test_acc, good_test_loss, bst = train_eval_fulltrain(\n",
        "        Xtr, ytr, Xte, yte, good_params, good_rounds\n",
        "    )\n",
        "\n",
        "    if good_test_acc < MIN_GOOD_TEST_ACC:\n",
        "        # cleanup\n",
        "        del bst, X, y, Xtr, Xte, ytr, yte\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # ============================================================\n",
        "    # Compute W7 using xgboost2ww.convert()\n",
        "    # ============================================================\n",
        "    try:\n",
        "        # If convert() can accept CSR directly, great.\n",
        "        # If it cannot, this will throw and we skip cleanly.\n",
        "        layer_W7 = convert(\n",
        "            model=bst,\n",
        "            data=Xtr,\n",
        "            labels=ytr,\n",
        "            W=\"W7\",\n",
        "            nfolds=NFOLDS,\n",
        "            t_points=T_TRAJ,\n",
        "            random_state=RNG,\n",
        "            train_params=good_params,\n",
        "            num_boost_round=good_rounds,\n",
        "            multiclass=\"error\",\n",
        "            return_type=\"torch\",\n",
        "            verbose=False,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"SKIP during convert():\", type(e).__name__, e)\n",
        "        # cleanup\n",
        "        del bst, X, y, Xtr, Xte, ytr, yte\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # WeightWatcher structural diagnostics\n",
        "    alpha_W7, traps_W7 = ww_alpha_traps_from_layer(layer_W7)\n",
        "\n",
        "    rows.append(dict(\n",
        "        openml_id=int(did_loaded),\n",
        "        dataset=name,\n",
        "        n_rows_total=int(X.shape[0]),\n",
        "        n_train=int(Xtr.shape[0]),\n",
        "        n_test=int(Xte.shape[0]),\n",
        "        n_features=int(X.shape[1]),\n",
        "        rounds=int(good_rounds),\n",
        "        cv_logloss=float(good_cv_logloss),\n",
        "        good_train_acc=float(good_train_acc),\n",
        "        good_test_acc=float(good_test_acc),\n",
        "        good_test_loss=float(good_test_loss),\n",
        "        alpha_W7=float(alpha_W7),\n",
        "        traps_W7=float(traps_W7),\n",
        "    ))\n",
        "\n",
        "    kept += 1\n",
        "    elapsed = (time.time() - t0) / 60.0\n",
        "\n",
        "    print(\n",
        "        f\"[{kept}/{TARGET_DATASETS}] {name} (OpenML {did_loaded}) \"\n",
        "        f\"| train/test={good_train_acc:.3f}/{good_test_acc:.3f} \"\n",
        "        f\"| α(W7)={alpha_W7:.2f} traps(W7)={traps_W7:.1f} \"\n",
        "        f\"| elapsed={elapsed:.1f} min\",\n",
        "        flush=True\n",
        "    )\n",
        "\n",
        "    # cleanup big objects each iteration\n",
        "    del bst, layer_W7, X, y, Xtr, Xte, ytr, yte\n",
        "    gc.collect()\n",
        "\n",
        "df_good = pd.DataFrame(rows)\n",
        "\n",
        "print(f\"\\nDONE. datasets_kept={df_good['openml_id'].nunique()} rows={len(df_good)}\")\n",
        "df_good"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP_ZQ6JaUC6k",
        "outputId": "b0907e2d-6618-4a12-b745-aa7878740a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50] kr-vs-kp (OpenML 3) | train/test=0.995/0.986 | α(W7)=2.28 traps(W7)=0.0 | elapsed=0.9 min\n",
            "[2/50] breast-w (OpenML 15) | train/test=0.977/0.979 | α(W7)=1.94 traps(W7)=2.0 | elapsed=1.2 min\n",
            "SKIP during convert(): IndexError tuple index out of range\n",
            "[3/50] credit-approval (OpenML 29) | train/test=0.918/0.870 | α(W7)=3.16 traps(W7)=0.0 | elapsed=2.5 min\n",
            "[4/50] credit-g (OpenML 31) | train/test=0.873/0.750 | α(W7)=3.86 traps(W7)=0.0 | elapsed=2.7 min\n",
            "[5/50] diabetes (OpenML 37) | train/test=0.842/0.766 | α(W7)=2.32 traps(W7)=0.0 | elapsed=2.9 min\n",
            "[6/50] spambase (OpenML 44) | train/test=0.982/0.953 | α(W7)=3.10 traps(W7)=0.0 | elapsed=3.7 min\n",
            "[7/50] tic-tac-toe (OpenML 50) | train/test=0.999/0.990 | α(W7)=2.65 traps(W7)=0.0 | elapsed=4.2 min\n",
            "[8/50] electricity (OpenML 151) | train/test=0.948/0.905 | α(W7)=2.49 traps(W7)=0.0 | elapsed=7.2 min\n",
            "[9/50] sick (OpenML 38) | train/test=0.992/0.985 | α(W7)=2.09 traps(W7)=0.0 | elapsed=7.6 min\n",
            "[10/50] scene (OpenML 312) | train/test=0.995/0.975 | α(W7)=2.19 traps(W7)=0.0 | elapsed=11.8 min\n",
            "[11/50] monks-problems-1 (OpenML 333) | train/test=1.000/1.000 | α(W7)=2.41 traps(W7)=0.0 | elapsed=12.0 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots\n",
        "\n",
        "These quick plots help you sanity-check:\n",
        "- α values across datasets for the W7 (default) matrix\n",
        "- traps across datasets for the W7 (default) matrix\n",
        "- relationship between holdout accuracy and α(W7)"
      ],
      "metadata": {
        "id": "9QWIYyx4UDBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if len(df_good) == 0:\n",
        "    print(\"No datasets kept. Try lowering MIN_GOOD_TEST_ACC.\")\n",
        "else:\n",
        "    x = np.arange(len(df_good))\n",
        "\n",
        "    # Alpha across datasets\n",
        "    plt.figure()\n",
        "    plt.plot(x, df_good[\"alpha_W7\"].values, label=\"W7\")\n",
        "    plt.xticks(x, df_good[\"dataset\"].values, rotation=90)\n",
        "    plt.ylabel(\"alpha\")\n",
        "    plt.title(\"Alpha across datasets\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Traps across datasets\n",
        "    plt.figure()\n",
        "    plt.plot(x, df_good[\"traps_W7\"].values, label=\"W7\")\n",
        "    plt.xticks(x, df_good[\"dataset\"].values, rotation=90)\n",
        "    plt.ylabel(\"traps (rand_num_spikes)\")\n",
        "    plt.title(\"Traps across datasets\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Holdout accuracy vs alpha(W7)\n",
        "    plt.figure()\n",
        "    plt.scatter(df_good[\"good_test_acc\"].values, df_good[\"alpha_W7\"].values)\n",
        "    plt.xlabel(\"Holdout test accuracy\")\n",
        "    plt.ylabel(\"alpha (W7)\")\n",
        "    plt.title(\"Holdout accuracy vs alpha(W7)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Tn5W9wt-UDI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H6CpaC73UJvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO65tbJpWxsNZrfIDzmlXJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}