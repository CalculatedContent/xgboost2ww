{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ1fzZCeRp1J"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CalculatedContent/xgboost2ww/blob/main/notebooks/XGBWW_Catalog_Random100_XGBoost_Accuracy_WithOverfitCatalog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# What this notebook does\n",
    "\n",
    "This notebook benchmarks **XGBoost classification accuracy** across a random sample of datasets from the project catalog, then intentionally runs a few **overfit configurations** to make failure modes visible.\n",
    "\n",
    "## Workflow at a glance\n",
    "- Load the dataset catalog from Drive and sample datasets per source using a fixed random seed.\n",
    "- Train/evaluate a baseline XGBoost model for each sampled dataset.\n",
    "- Run a targeted set of overfit modes (for example: very deep trees, too many rounds, reduced regularization, tiny train split, or leakage).\n",
    "- Save progress to checkpoint files so interrupted runs can resume.\n",
    "- Aggregate results into one table that includes both normal benchmark runs and overfit-case runs.\n",
    "\n",
    "## Why include overfit cases?\n",
    "Including overfit runs helps you compare \"good\" generalization behavior against intentionally bad behavior in the same experiment output, which makes diagnostics and sanity checks easier.\n",
    "\n",
    "## Expected outputs\n",
    "- Per-dataset training/evaluation metrics (including held-out accuracy).\n",
    "- Metadata about model settings and overfit mode (if used).\n",
    "- A combined checkpoint/results artifact for downstream analysis.\n"
   ],
   "id": "BZ1fzZCeRp1J"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XlqyD7nRp1K"
   },
   "source": [
    "# XGBWW catalog-driven random-per-source XGBoost benchmark + targeted overfit cases\n",
    "\n",
    "This notebook keeps the original catalog benchmark workflow, then adds a small set of intentionally overfit models (5\u20136 cases) and writes an aggregated checkpoint with both good and overfit results.\n"
   ],
   "id": "7XlqyD7nRp1K"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez6YDyX9Rp1L"
   },
   "source": [
    "## 1) Mount Google Drive and configure paths\n"
   ],
   "id": "Ez6YDyX9Rp1L"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DffIfgUGRp1L",
    "outputId": "8a4a0f70-708f-4023-8d50-39a51545aeb4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Catalog path: /content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv\n",
      "Experiment checkpoint: /content/drive/MyDrive/xgbwwdata/experiment_checkpoints/random500_xgboost_accuracy_plus_overfit_run01\n",
      "Restart mode: True\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ===== USER CONFIG =====\n",
    "CATALOG_CSV = Path(\"/content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv\")\n",
    "RANDOM_SEED = 42\n",
    "RANDOM_SAMPLE_SIZE = 500\n",
    "TEST_SIZE = 0.20\n",
    "EXPERIMENT_ROOT = Path(\"/content/drive/MyDrive/xgbwwdata/experiment_checkpoints\")\n",
    "DEFAULT_EXPERIMENT_BASENAME = \"random500_xgboost_accuracy_plus_overfit\"\n",
    "\n",
    "# Targeted overfit cases\n",
    "OVERFIT_MODES = [\n",
    "    \"deep_trees\",\n",
    "    \"too_many_rounds\",\n",
    "    \"no_regularization\",\n",
    "    \"no_subsampling\",\n",
    "    \"tiny_trainset\",\n",
    "    \"leakage\",\n",
    "]\n",
    "MAX_OVERFIT_CASES = 6\n",
    "TINY_TRAIN_FRAC = 0.20\n",
    "\n",
    "# Restart control\n",
    "RESTART_EXPERIMENT = True\n",
    "RETRY_FAILED_DATASETS = False  # Default: do not retry failed/model_failed datasets on restart.\n",
    "EXPERIMENT_NAME = \"random500_xgboost_accuracy_plus_overfit_run01\" # Required for restart.\n",
    "AUTO_INCREMENT_IF_NAME_MISSING = True\n",
    "# =======================\n",
    "\n",
    "\n",
    "def next_experiment_name(root: Path, base_name: str) -> str:\n",
    "    existing = [d.name for d in root.glob(f\"{base_name}_run*\") if d.is_dir()]\n",
    "    nums = []\n",
    "    for name in existing:\n",
    "        suffix = name.replace(f\"{base_name}_run\", \"\")\n",
    "        if suffix.isdigit():\n",
    "            nums.append(int(suffix))\n",
    "    n = (max(nums) + 1) if nums else 1\n",
    "    return f\"{base_name}_run{n:02d}\"\n",
    "\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "EXPERIMENT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if RESTART_EXPERIMENT:\n",
    "    if not EXPERIMENT_NAME:\n",
    "        raise ValueError(\"Set EXPERIMENT_NAME when RESTART_EXPERIMENT=True.\")\n",
    "    EXPERIMENT_ID = EXPERIMENT_NAME\n",
    "else:\n",
    "    if EXPERIMENT_NAME:\n",
    "        EXPERIMENT_ID = EXPERIMENT_NAME\n",
    "    elif AUTO_INCREMENT_IF_NAME_MISSING:\n",
    "        EXPERIMENT_ID = next_experiment_name(EXPERIMENT_ROOT, DEFAULT_EXPERIMENT_BASENAME)\n",
    "    else:\n",
    "        EXPERIMENT_ID = DEFAULT_EXPERIMENT_BASENAME\n",
    "\n",
    "CHECKPOINT_DIR = EXPERIMENT_ROOT / EXPERIMENT_ID\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_RESULTS_CSV = CHECKPOINT_DIR / \"checkpoint_results.csv\"\n",
    "CHECKPOINT_ERRORS_CSV = CHECKPOINT_DIR / \"errors.csv\"\n",
    "CHECKPOINT_AGGREGATED_CSV = CHECKPOINT_DIR / \"checkpoint_results_good_plus_overfit.csv\"\n",
    "SELECTED_DATASETS_CSV = CHECKPOINT_DIR / \"selected_datasets.csv\"\n",
    "EXPERIMENT_CONFIG_JSON = CHECKPOINT_DIR / \"experiment_config.json\"\n",
    "\n",
    "print(\"Catalog path:\", CATALOG_CSV)\n",
    "print(\"Experiment checkpoint:\", CHECKPOINT_DIR)\n",
    "print(\"Restart mode:\", RESTART_EXPERIMENT)\n",
    "\n",
    "\n"
   ],
   "id": "DffIfgUGRp1L"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How far did we get before ?\n"
   ],
   "metadata": {
    "id": "tLSqXOADsgI_"
   },
   "id": "tLSqXOADsgI_"
  },
  {
   "cell_type": "code",
   "source": [
    "# Progress snapshot from Google Drive checkpoint.\n",
    "# Restart behavior:\n",
    "# - status == \"completed\" is always skipped\n",
    "# - failed/model_failed are retried only when RETRY_FAILED_DATASETS=True\n",
    "# - pending/missing are always run\n",
    "selected_df = pd.read_csv(SELECTED_DATASETS_CSV) if SELECTED_DATASETS_CSV.exists() else None\n",
    "checkpoint_df = pd.read_csv(CHECKPOINT_RESULTS_CSV) if CHECKPOINT_RESULTS_CSV.exists() else pd.DataFrame()\n",
    "\n",
    "if selected_df is not None and \"dataset_uid\" in selected_df.columns:\n",
    "    target_uids = selected_df[\"dataset_uid\"].astype(str).tolist()\n",
    "else:\n",
    "    target_uids = checkpoint_df.get(\"dataset_uid\", pd.Series(dtype=str)).astype(str).tolist()\n",
    "    if not target_uids:\n",
    "        target_uids = [None] * RANDOM_SAMPLE_SIZE\n",
    "\n",
    "status_by_uid = {}\n",
    "if not checkpoint_df.empty and \"dataset_uid\" in checkpoint_df.columns:\n",
    "    checkpoint_df = checkpoint_df.drop_duplicates(subset=[\"dataset_uid\"], keep=\"last\")\n",
    "    if \"status\" in checkpoint_df.columns:\n",
    "        status_by_uid = dict(zip(checkpoint_df[\"dataset_uid\"].astype(str), checkpoint_df[\"status\"].astype(str)))\n",
    "\n",
    "completed_models = 0\n",
    "remaining_models = 0\n",
    "next_uid = None\n",
    "next_index = None\n",
    "\n",
    "for idx, uid in enumerate(target_uids, start=1):\n",
    "    uid_key = str(uid) if uid is not None else None\n",
    "    status = status_by_uid.get(uid_key, \"missing\")\n",
    "\n",
    "    if status == \"completed\":\n",
    "        completed_models += 1\n",
    "        should_run = False\n",
    "    elif status in {\"failed\", \"model_failed\"}:\n",
    "        should_run = RETRY_FAILED_DATASETS\n",
    "    else:\n",
    "        should_run = True\n",
    "\n",
    "    if should_run:\n",
    "        remaining_models += 1\n",
    "        if next_uid is None:\n",
    "            next_uid = uid_key\n",
    "            next_index = idx\n",
    "\n",
    "failed_statuses = {\"failed\", \"model_failed\"}\n",
    "failed_models = sum(1 for uid in target_uids if status_by_uid.get(str(uid), \"missing\") in failed_statuses)\n",
    "\n",
    "print(f\"Total selected models: {len(target_uids)}\")\n",
    "print(f\"Completed models: {completed_models}\")\n",
    "print(f\"Failed models currently on checkpoint: {failed_models}\")\n",
    "print(f\"Retry failed datasets on restart: {RETRY_FAILED_DATASETS}\")\n",
    "print(f\"Remaining models to run on restart: {remaining_models}\")\n",
    "if next_uid is not None:\n",
    "    print(f\"Restart will resume at dataset #{next_index}: {next_uid}\")\n",
    "else:\n",
    "    print(\"Checkpoint is fully completed. No more models remain.\")"
   ],
   "metadata": {
    "id": "Z34Aur-qsgQQ",
    "outputId": "3471e7f9-afde-4a24-bf41-25c62cfe2bc1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "Z34Aur-qsgQQ",
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total selected models: 500\n",
      "Completed models: 0\n",
      "Failed models currently on checkpoint: 0\n",
      "Retry failed datasets on restart: False\n",
      "Remaining models to run on restart: 500\n",
      "Restart will resume at dataset #1: openml:75\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT9KuFpNRp1L"
   },
   "source": [
    "## 2) Install dependencies\n",
    "\n",
    "Use the same repository-install flow as the other Colab notebooks (no `pip install xgbwwdata`).\n"
   ],
   "id": "aT9KuFpNRp1L"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4cFzOrOvRp1M",
    "outputId": "17c8265f-2459-47a9-db88-0a2a39c42441",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into '/content/repo_xgbwwdata'...\n",
      "remote: Enumerating objects: 142, done.\u001b[K\n",
      "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
      "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
      "remote: Total 142 (delta 11), reused 0 (delta 0), pack-reused 100 (from 2)\u001b[K\n",
      "Receiving objects: 100% (142/142), 233.90 KiB | 1.96 MiB/s, done.\n",
      "Resolving deltas: 100% (48/48), done.\n",
      "+ /usr/bin/python3 -m pip install -U pip setuptools wheel\n",
      "+ /usr/bin/python3 -m pip install -r /content/repo_xgbwwdata/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Install xgbwwdata from a fresh clone using the repository installer script\n",
    "!rm -rf /content/repo_xgbwwdata\n",
    "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\n",
    "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\n",
    "\n",
    "# Notebook-specific dependencies\n",
    "%pip install -q openml pmlb keel-ds xgboost scikit-learn xgboost2ww weightwatcher\n"
   ],
   "id": "4cFzOrOvRp1M"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM2nQyMGRp1M"
   },
   "source": [
    "## 3) Imports\n"
   ],
   "id": "FM2nQyMGRp1M"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p3LUiyt9Rp1M"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import weightwatcher as ww\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgbwwdata import Filters, load_dataset\n",
    "from xgboost2ww import convert\n"
   ],
   "id": "p3LUiyt9Rp1M"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxUyvl08Rp1M"
   },
   "source": [
    "## 4) Load catalog and pick 100 random dataset UIDs (checkpoint-aware)\n"
   ],
   "id": "sxUyvl08Rp1M"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R-esy_yzRp1M"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not CATALOG_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Catalog not found: {CATALOG_CSV}. Run XGBWW_Dataset_Catalog_Checkpoint.ipynb first.\")\n",
    "\n",
    "df_catalog = pd.read_csv(CATALOG_CSV)\n",
    "print(\"Catalog shape:\", df_catalog.shape)\n",
    "\n",
    "required_cols = {\"dataset_uid\", \"source\", \"task_type\"}\n",
    "missing = required_cols - set(df_catalog.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Catalog is missing required columns: {missing}\")\n",
    "\n",
    "# Accuracy is for classification; keep classification-like tasks\n",
    "df_cls = df_catalog[df_catalog[\"task_type\"].astype(str).str.contains(\"classification\", case=False, na=False)].copy()\n",
    "if df_cls.empty:\n",
    "    raise ValueError(\"No classification datasets found in catalog.\")\n",
    "\n",
    "# Select 100 random dataset UIDs up front for this experiment.\n",
    "n_select = min(RANDOM_SAMPLE_SIZE, len(df_cls))\n",
    "df_pick = df_cls.sample(n=n_select, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Selected datasets:\", len(df_pick))\n",
    "display(df_pick[[\"source\", \"dataset_uid\", \"name\", \"task_type\"]].sort_values([\"source\", \"dataset_uid\"]))\n",
    "\n",
    "# Initialize or reload checkpoint table\n",
    "if RESTART_EXPERIMENT and CHECKPOINT_RESULTS_CSV.exists():\n",
    "    checkpoint_df = pd.read_csv(CHECKPOINT_RESULTS_CSV)\n",
    "    print(f\"Loaded existing checkpoint rows: {len(checkpoint_df)}\")\n",
    "else:\n",
    "    checkpoint_df = df_pick.copy()\n",
    "    checkpoint_df[\"status\"] = \"pending\"\n",
    "    checkpoint_df[\"error_message\"] = \"\"\n",
    "\n",
    "    # model / hyperparameter fields (blank at initialization)\n",
    "    checkpoint_df[\"xgboost_params\"] = \"\"\n",
    "    checkpoint_df[\"rounds\"] = np.nan\n",
    "    checkpoint_df[\"n_classes\"] = np.nan\n",
    "    checkpoint_df[\"train_size\"] = np.nan\n",
    "    checkpoint_df[\"test_size_rows\"] = np.nan\n",
    "\n",
    "    # train/test accuracy fields\n",
    "    checkpoint_df[\"train_accuracy\"] = np.nan\n",
    "    checkpoint_df[\"test_accuracy\"] = np.nan\n",
    "\n",
    "    # weightwatcher metrics\n",
    "    checkpoint_df[\"alpha\"] = np.nan\n",
    "    checkpoint_df[\"ERG_gap\"] = np.nan\n",
    "    checkpoint_df[\"num_traps\"] = np.nan\n",
    "\n",
    "    # metadata from actual loaded data\n",
    "    checkpoint_df[\"dataset_name\"] = checkpoint_df.get(\"name\", \"\")\n",
    "    checkpoint_df[\"dataset_openml_id\"] = checkpoint_df.get(\"openml_data_id\", np.nan)\n",
    "    checkpoint_df[\"n_rows\"] = np.nan\n",
    "    checkpoint_df[\"n_features\"] = np.nan\n",
    "    checkpoint_df[\"test_size\"] = TEST_SIZE\n",
    "\n",
    "# Ensure the selected datasets are saved for restart reproducibility\n",
    "if not SELECTED_DATASETS_CSV.exists():\n",
    "    df_pick.to_csv(SELECTED_DATASETS_CSV, index=False)\n",
    "\n",
    "# If restarting, trust existing selected datasets file when present\n",
    "if RESTART_EXPERIMENT and SELECTED_DATASETS_CSV.exists():\n",
    "    df_pick = pd.read_csv(SELECTED_DATASETS_CSV)\n",
    "\n",
    "# Save initial checkpoint immediately\n",
    "checkpoint_df.to_csv(CHECKPOINT_RESULTS_CSV, index=False)\n",
    "print(\"Checkpoint file:\", CHECKPOINT_RESULTS_CSV)\n"
   ],
   "id": "R-esy_yzRp1M"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsD9mQFdRp1M"
   },
   "source": [
    "## 5) Train one XGBoost model per sampled dataset, run xgboost2ww + WeightWatcher, and report metrics\n"
   ],
   "id": "tsD9mQFdRp1M"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uhd4vPK7Rp1N"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "filters = Filters(\n",
    "    min_rows=200,\n",
    "    max_rows=60000,\n",
    "    max_features=50000,\n",
    "    max_dense_elements=int(2e8),\n",
    ")\n",
    "\n",
    "\n",
    "class ModelTrainingFailed(RuntimeError):\n",
    "    \"\"\"Raised when XGBoost model fitting fails for a dataset.\"\"\"\n",
    "\n",
    "\n",
    "def detect_xgb_compute_params():\n",
    "    \"\"\"Prefer CUDA in Colab when available; gracefully fall back to CPU.\"\"\"\n",
    "    gpu_params = {\"tree_method\": \"hist\", \"device\": \"cuda\"}\n",
    "    cpu_params = {\"tree_method\": \"hist\"}\n",
    "\n",
    "    X_probe = np.array([[0.0], [1.0], [2.0], [3.0]], dtype=np.float32)\n",
    "    y_probe = np.array([0, 0, 1, 1], dtype=np.float32)\n",
    "    dprobe = xgb.DMatrix(X_probe, label=y_probe)\n",
    "\n",
    "    try:\n",
    "        xgb.train(\n",
    "            params={\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"eval_metric\": \"logloss\",\n",
    "                **gpu_params,\n",
    "            },\n",
    "            dtrain=dprobe,\n",
    "            num_boost_round=1,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        return gpu_params, \"gpu\"\n",
    "    except Exception:\n",
    "        return cpu_params, \"cpu\"\n",
    "\n",
    "\n",
    "def apply_overfit_mode(params: dict, mode: str, seed: int):\n",
    "    p = dict(params)\n",
    "    rng = np.random.default_rng(seed + 999)\n",
    "\n",
    "    if mode == \"deep_trees\":\n",
    "        p[\"max_depth\"] = int(rng.integers(10, 16))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.1, 0.3))\n",
    "    elif mode == \"too_many_rounds\":\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.15, 0.35))\n",
    "        p[\"max_depth\"] = int(rng.integers(4, 8))\n",
    "    elif mode == \"no_regularization\":\n",
    "        p[\"reg_lambda\"] = 0.0\n",
    "        p[\"reg_alpha\"] = 0.0\n",
    "        p[\"gamma\"] = 0.0\n",
    "        p[\"max_depth\"] = int(rng.integers(5, 9))\n",
    "    elif mode == \"no_subsampling\":\n",
    "        p[\"subsample\"] = 1.0\n",
    "        p[\"colsample_bytree\"] = 1.0\n",
    "        p[\"max_depth\"] = int(rng.integers(4, 8))\n",
    "    elif mode == \"tiny_trainset\":\n",
    "        p[\"max_depth\"] = int(rng.integers(6, 12))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.15, 0.35))\n",
    "    elif mode == \"leakage\":\n",
    "        p[\"max_depth\"] = int(rng.integers(3, 6))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.05, 0.2))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown overfit mode: {mode}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def training_schedule(case_type: str, overfit_mode: str | None):\n",
    "    if case_type == \"overfit\" and overfit_mode == \"too_many_rounds\":\n",
    "        return 4000, 400\n",
    "    if case_type == \"overfit\":\n",
    "        return 1000, 120\n",
    "    return 1200, None\n",
    "\n",
    "\n",
    "XGB_COMPUTE_PARAMS, XGB_COMPUTE_BACKEND = detect_xgb_compute_params()\n",
    "print(f\"XGBoost compute backend detected: {XGB_COMPUTE_BACKEND} | params={XGB_COMPUTE_PARAMS}\")\n",
    "\n",
    "\n",
    "def fit_and_score(row_data: dict, case_type: str = \"good\", overfit_mode: str | None = None, seed_offset: int = 0):\n",
    "    dataset_uid = row_data[\"dataset_uid\"]\n",
    "    source = row_data[\"source\"]\n",
    "    local_seed = RANDOM_SEED + seed_offset\n",
    "\n",
    "    X, y, meta = load_dataset(dataset_uid, filters=filters)\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    classes, y_enc = np.unique(y, return_inverse=True)\n",
    "    n_classes = len(classes)\n",
    "    if n_classes < 2:\n",
    "        raise ValueError(f\"Dataset {dataset_uid} has <2 classes after loading.\")\n",
    "\n",
    "    stratify = y_enc if n_classes > 1 else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_enc, test_size=TEST_SIZE, random_state=local_seed, stratify=stratify\n",
    "    )\n",
    "\n",
    "    if case_type == \"overfit\" and overfit_mode == \"tiny_trainset\":\n",
    "        sub_idx, _ = train_test_split(\n",
    "            np.arange(len(y_train)),\n",
    "            test_size=(1.0 - TINY_TRAIN_FRAC),\n",
    "            random_state=local_seed,\n",
    "            stratify=y_train,\n",
    "        )\n",
    "        X_train, y_train = X_train[sub_idx], y_train[sub_idx]\n",
    "\n",
    "    if case_type == \"overfit\" and overfit_mode == \"leakage\":\n",
    "        rng = np.random.default_rng(local_seed)\n",
    "        Xtr_dense = X_train.toarray() if hasattr(X_train, \"toarray\") else np.asarray(X_train)\n",
    "        Xte_dense = X_test.toarray() if hasattr(X_test, \"toarray\") else np.asarray(X_test)\n",
    "        leak_tr = (y_train + 0.05 * rng.standard_normal(len(y_train))).astype(np.float32).reshape(-1, 1)\n",
    "        leak_te = (y_test + 0.05 * rng.standard_normal(len(y_test))).astype(np.float32).reshape(-1, 1)\n",
    "        X_train = np.hstack([Xtr_dense.astype(np.float32), leak_tr])\n",
    "        X_test = np.hstack([Xte_dense.astype(np.float32), leak_te])\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    try:\n",
    "        if n_classes == 2:\n",
    "            params = {\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"eval_metric\": \"logloss\",\n",
    "                **XGB_COMPUTE_PARAMS,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"max_depth\": 6,\n",
    "                \"subsample\": 0.85,\n",
    "                \"colsample_bytree\": 0.85,\n",
    "                \"min_child_weight\": 2.0,\n",
    "                \"reg_lambda\": 2.0,\n",
    "                \"reg_alpha\": 0.2,\n",
    "                \"gamma\": 0.1,\n",
    "                \"seed\": local_seed,\n",
    "            }\n",
    "            if case_type == \"overfit\":\n",
    "                params = apply_overfit_mode(params, overfit_mode, local_seed)\n",
    "            num_boost_round, early_stop = training_schedule(case_type, overfit_mode)\n",
    "            if early_stop is None:\n",
    "                cv = xgb.cv(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    early_stopping_rounds=50,\n",
    "                    seed=local_seed,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = len(cv)\n",
    "                model = xgb.train(params=params, dtrain=dtrain, num_boost_round=rounds, verbose_eval=False)\n",
    "            else:\n",
    "                model = xgb.train(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "                    early_stopping_rounds=early_stop,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = int(model.best_iteration + 1)\n",
    "\n",
    "            yhat_tr = (model.predict(dtrain) >= 0.5).astype(int)\n",
    "            yhat_te = (model.predict(dtest) >= 0.5).astype(int)\n",
    "        else:\n",
    "            params = {\n",
    "                \"objective\": \"multi:softprob\",\n",
    "                \"num_class\": n_classes,\n",
    "                \"eval_metric\": \"mlogloss\",\n",
    "                **XGB_COMPUTE_PARAMS,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"max_depth\": 7,\n",
    "                \"subsample\": 0.9,\n",
    "                \"colsample_bytree\": 0.9,\n",
    "                \"min_child_weight\": 1.0,\n",
    "                \"reg_lambda\": 1.0,\n",
    "                \"reg_alpha\": 0.1,\n",
    "                \"gamma\": 0.0,\n",
    "                \"seed\": local_seed,\n",
    "            }\n",
    "            if case_type == \"overfit\":\n",
    "                params = apply_overfit_mode(params, overfit_mode, local_seed)\n",
    "            num_boost_round, early_stop = training_schedule(case_type, overfit_mode)\n",
    "            if early_stop is None:\n",
    "                cv = xgb.cv(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    early_stopping_rounds=60,\n",
    "                    seed=local_seed,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = len(cv)\n",
    "                model = xgb.train(params=params, dtrain=dtrain, num_boost_round=rounds, verbose_eval=False)\n",
    "            else:\n",
    "                model = xgb.train(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "                    early_stopping_rounds=early_stop,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = int(model.best_iteration + 1)\n",
    "\n",
    "            yhat_tr = np.argmax(model.predict(dtrain), axis=1)\n",
    "            yhat_te = np.argmax(model.predict(dtest), axis=1)\n",
    "    except Exception as e:\n",
    "        raise ModelTrainingFailed(f\"Model training failed for dataset {dataset_uid}: {e}\") from e\n",
    "\n",
    "    ww_layer = convert(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        W=\"W7\",\n",
    "        nfolds=5,\n",
    "        t_points=160,\n",
    "        random_state=local_seed,\n",
    "        train_params=params,\n",
    "        num_boost_round=rounds,\n",
    "        multiclass=\"avg\" if n_classes > 2 else \"error\",\n",
    "        return_type=\"torch\",\n",
    "    )\n",
    "    watcher = ww.WeightWatcher(model=ww_layer)\n",
    "    details_df = watcher.analyze(ERG=True, randomize=True, plot=False)\n",
    "\n",
    "    alpha = float(details_df[\"alpha\"].iloc[0]) if \"alpha\" in details_df else np.nan\n",
    "    erg_gap = float(details_df[\"ERG_gap\"].iloc[0]) if \"ERG_gap\" in details_df else np.nan\n",
    "    num_traps = float(details_df[\"num_traps\"].iloc[0]) if \"num_traps\" in details_df else np.nan\n",
    "\n",
    "    result = {\n",
    "        \"source\": source,\n",
    "        \"dataset_uid\": dataset_uid,\n",
    "        \"dataset_name\": row_data.get(\"name\", meta.get(\"name\")),\n",
    "        \"task_type\": row_data.get(\"task_type\"),\n",
    "        \"dataset_openml_id\": row_data.get(\"openml_data_id\"),\n",
    "        \"n_rows\": int(meta.get(\"n_rows\", len(y))),\n",
    "        \"n_features\": int(meta.get(\"n_features\", X.shape[1] if hasattr(X, \"shape\") else -1)),\n",
    "        \"n_classes\": int(n_classes),\n",
    "        \"test_size\": float(TEST_SIZE),\n",
    "        \"train_size\": int(len(y_train)),\n",
    "        \"test_size_rows\": int(len(y_test)),\n",
    "        \"rounds\": int(rounds),\n",
    "        \"train_accuracy\": float(accuracy_score(y_train, yhat_tr)),\n",
    "        \"test_accuracy\": float(accuracy_score(y_test, yhat_te)),\n",
    "        \"accuracy_gap\": float(accuracy_score(y_train, yhat_tr) - accuracy_score(y_test, yhat_te)),\n",
    "        \"alpha\": alpha,\n",
    "        \"ERG_gap\": erg_gap,\n",
    "        \"num_traps\": num_traps,\n",
    "        \"case_type\": case_type,\n",
    "        \"overfit_mode\": overfit_mode if overfit_mode else \"none\",\n",
    "        \"seed_used\": int(local_seed),\n",
    "        \"xgboost_params\": json.dumps(params, sort_keys=True),\n",
    "        \"status\": \"completed\",\n",
    "        \"error_message\": \"\",\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_checkpoint_row(uid: str, updates: dict):\n",
    "    global checkpoint_df\n",
    "    mask = checkpoint_df[\"dataset_uid\"] == uid\n",
    "    if not mask.any():\n",
    "        return\n",
    "    for k, v in updates.items():\n",
    "        if k not in checkpoint_df.columns:\n",
    "            checkpoint_df[k] = np.nan\n",
    "        checkpoint_df.loc[mask, k] = v\n",
    "    checkpoint_df.to_csv(CHECKPOINT_RESULTS_CSV, index=False)\n",
    "\n",
    "\n",
    "total_models = len(df_pick)\n",
    "\n",
    "for model_idx, row in enumerate(df_pick.itertuples(index=False), start=1):\n",
    "    row_data = row._asdict()\n",
    "    uid = row_data[\"dataset_uid\"]\n",
    "    progress = f\"[{model_idx}/{total_models}]\"\n",
    "\n",
    "    existing = checkpoint_df.loc[checkpoint_df[\"dataset_uid\"] == uid, \"status\"]\n",
    "    if not existing.empty:\n",
    "        existing_status = str(existing.iloc[0])\n",
    "        if existing_status == \"completed\":\n",
    "            print(f\"{progress} Skipping completed dataset: {uid}\")\n",
    "            continue\n",
    "        if existing_status in {\"failed\", \"model_failed\"} and not RETRY_FAILED_DATASETS:\n",
    "            print(f\"{progress} Skipping failed dataset (retry disabled): {uid}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"{progress} Training + WW analysis: {uid}\")\n",
    "    try:\n",
    "        result = fit_and_score(row_data, case_type=\"good\", overfit_mode=None, seed_offset=0)\n",
    "        update_checkpoint_row(uid, result)\n",
    "        print(f\"{progress} Completed: {uid}\")\n",
    "    except ModelTrainingFailed as e:\n",
    "        err_msg = str(e)\n",
    "        update_checkpoint_row(uid, {\"status\": \"model_failed\", \"error_message\": err_msg})\n",
    "        print(f\"{progress} Marked model_failed for {uid}: {err_msg}\")\n",
    "    except Exception as e:\n",
    "        err_msg = str(e)\n",
    "        update_checkpoint_row(uid, {\"status\": \"failed\", \"error_message\": err_msg})\n",
    "        print(f\"{progress} Skipped {uid}: {err_msg}\")\n",
    "\n",
    "results_df = checkpoint_df[checkpoint_df[\"status\"] == \"completed\"].copy()\n",
    "errors_df = checkpoint_df[checkpoint_df[\"status\"] != \"completed\"][[\"source\", \"dataset_uid\", \"status\", \"error_message\"]].rename(columns={\"error_message\": \"error\"}).copy()\n",
    "\n",
    "if not errors_df.empty:\n",
    "    errors_df.to_csv(CHECKPOINT_ERRORS_CSV, index=False)\n",
    "\n",
    "print(\"Completed:\", len(results_df), \"datasets\")\n",
    "print(\"Failed:\", len(errors_df), \"datasets\")\n",
    "\n",
    "display(results_df.sort_values([\"source\", \"test_accuracy\"], ascending=[True, False]))\n"
   ],
   "id": "uhd4vPK7Rp1N"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qma8FIOpRp1N"
   },
   "source": [
    "## 6) Summary tables (accuracy + WeightWatcher metrics)\n"
   ],
   "id": "qma8FIOpRp1N"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lfZrM9vORp1N"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if results_df.empty:\n",
    "    print(\"No successful trainings.\")\n",
    "else:\n",
    "    metric_cols = [\"train_accuracy\", \"test_accuracy\", \"accuracy_gap\", \"alpha\", \"ERG_gap\", \"num_traps\"]\n",
    "    summary = (\n",
    "        results_df.groupby(\"source\", as_index=False)[metric_cols]\n",
    "        .agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "    )\n",
    "    summary.columns = [\"source\"] + [f\"{a}_{b}\" for a, b in summary.columns.tolist()[1:]]\n",
    "\n",
    "    print(\"Per-dataset results (good models):\")\n",
    "    display(results_df.sort_values([\"source\", \"test_accuracy\"], ascending=[True, False]))\n",
    "\n",
    "    print(\"Per-source summary (good models):\")\n",
    "    display(summary.sort_values(\"test_accuracy_mean\", ascending=False))\n",
    "\n",
    "    experiment_config = {\n",
    "        \"experiment_id\": EXPERIMENT_ID,\n",
    "        \"experiment_name\": EXPERIMENT_ID,\n",
    "        \"restart_experiment\": RESTART_EXPERIMENT,\n",
    "        \"catalog_csv\": str(CATALOG_CSV),\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"random_sample_size\": RANDOM_SAMPLE_SIZE,\n",
    "        \"test_size\": TEST_SIZE,\n",
    "        \"overfit_modes\": OVERFIT_MODES,\n",
    "        \"max_overfit_cases\": MAX_OVERFIT_CASES,\n",
    "        \"selected_dataset_count\": int(len(df_pick)),\n",
    "        \"successful_dataset_count\": int(len(results_df)),\n",
    "        \"failed_dataset_count\": int(len(errors_df)),\n",
    "    }\n",
    "\n",
    "    results_path = CHECKPOINT_DIR / \"results_per_dataset.csv\"\n",
    "    summary_path = CHECKPOINT_DIR / \"results_summary_by_source.csv\"\n",
    "\n",
    "    checkpoint_df.to_csv(CHECKPOINT_RESULTS_CSV, index=False)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    errors_df.to_csv(CHECKPOINT_ERRORS_CSV, index=False)\n",
    "    with open(EXPERIMENT_CONFIG_JSON, \"w\") as f:\n",
    "        json.dump(experiment_config, f, indent=2)\n",
    "\n",
    "    print(\"Saved checkpoint files:\")\n",
    "    print(\" -\", CHECKPOINT_RESULTS_CSV)\n",
    "    print(\" -\", results_path)\n",
    "    print(\" -\", summary_path)\n",
    "    print(\" -\", CHECKPOINT_ERRORS_CSV)\n",
    "    print(\" -\", EXPERIMENT_CONFIG_JSON)\n"
   ],
   "id": "lfZrM9vORp1N"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZVL3LcURp1N"
   },
   "source": [
    "## 7) Accuracy comparison plots vs WeightWatcher metrics\n"
   ],
   "id": "VZVL3LcURp1N"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVlB0ke9Rp1N"
   },
   "outputs": [],
   "source": [
    "if results_df.empty:\n",
    "    print(\"No successful trainings to plot.\")\n",
    "else:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plot_df = results_df.sort_values([\"source\", \"dataset_uid\"]).copy()\n",
    "\n",
    "    metrics = [\"alpha\", \"ERG_gap\", \"num_traps\"]\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 5), squeeze=False)\n",
    "\n",
    "    for ax, metric in zip(axes[0], metrics):\n",
    "        ax.scatter(plot_df[metric], plot_df[\"train_accuracy\"], label=\"Train accuracy\", alpha=0.8)\n",
    "        ax.scatter(plot_df[metric], plot_df[\"test_accuracy\"], label=\"Test accuracy\", alpha=0.8)\n",
    "        ax.set_xlabel(metric)\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.set_ylim(0.4, 1.05)\n",
    "        if metric == \"alpha\":\n",
    "            ax.set_xlim(1.5, 6)\n",
    "        ax.set_title(f\"Good models: Accuracy vs {metric}\")\n",
    "        ax.grid(alpha=0.2)\n",
    "\n",
    "    axes[0, 0].legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "wVlB0ke9Rp1N"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8) Train targeted overfit cases and build aggregated checkpoint\n"
   ],
   "metadata": {
    "id": "qDmk6e5iAkpL"
   },
   "id": "qDmk6e5iAkpL"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No completed good models. Cannot build overfit comparison set.\")\n",
    "    combined_df = results_df.copy()\n",
    "    overfit_df = pd.DataFrame()\n",
    "else:\n",
    "    overfit_rows = []\n",
    "    good_candidates = results_df.sort_values([\"test_accuracy\", \"dataset_uid\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    n_cases = min(MAX_OVERFIT_CASES, len(good_candidates), len(OVERFIT_MODES))\n",
    "\n",
    "    print(f\"Creating {n_cases} targeted overfit cases using modes: {OVERFIT_MODES[:n_cases]}\")\n",
    "    for idx in range(n_cases):\n",
    "        row_data = good_candidates.iloc[idx].to_dict()\n",
    "        mode = OVERFIT_MODES[idx]\n",
    "        uid = row_data[\"dataset_uid\"]\n",
    "        print(f\"[overfit {idx+1}/{n_cases}] dataset={uid} mode={mode}\")\n",
    "        try:\n",
    "            overfit_result = fit_and_score(\n",
    "                row_data,\n",
    "                case_type=\"overfit\",\n",
    "                overfit_mode=mode,\n",
    "                seed_offset=1000 + idx,\n",
    "            )\n",
    "            overfit_rows.append(overfit_result)\n",
    "        except Exception as e:\n",
    "            print(f\"  SKIP overfit case dataset={uid} mode={mode}: {e}\")\n",
    "\n",
    "    overfit_df = pd.DataFrame(overfit_rows)\n",
    "\n",
    "    good_df = results_df.copy()\n",
    "    good_df[\"case_type\"] = good_df.get(\"case_type\", \"good\")\n",
    "    good_df[\"overfit_mode\"] = good_df.get(\"overfit_mode\", \"none\")\n",
    "    if \"accuracy_gap\" not in good_df.columns:\n",
    "        good_df[\"accuracy_gap\"] = good_df[\"train_accuracy\"] - good_df[\"test_accuracy\"]\n",
    "\n",
    "    combined_df = pd.concat([good_df, overfit_df], ignore_index=True, sort=False)\n",
    "    combined_df.to_csv(CHECKPOINT_AGGREGATED_CSV, index=False)\n",
    "\n",
    "    print(\"Saved aggregated checkpoint:\")\n",
    "    print(\" -\", CHECKPOINT_AGGREGATED_CSV)\n",
    "    print(\"Good rows:\", len(good_df), \"| Overfit rows:\", len(overfit_df), \"| Total:\", len(combined_df))\n",
    "\n",
    "    display(\n",
    "        combined_df[\n",
    "            [\n",
    "                \"case_type\", \"overfit_mode\", \"source\", \"dataset_uid\", \"train_accuracy\", \"test_accuracy\",\n",
    "                \"accuracy_gap\", \"alpha\", \"ERG_gap\", \"num_traps\", \"rounds\"\n",
    "            ]\n",
    "        ].sort_values([\"case_type\", \"accuracy_gap\"], ascending=[True, False]).head(30)\n",
    "    )\n",
    "\n",
    "    # Extended comparison plots including num_traps and accuracy gap.\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    plot_metrics = [\"alpha\", \"ERG_gap\", \"num_traps\", \"accuracy_gap\"]\n",
    "\n",
    "    for ax, metric in zip(axes.flatten(), plot_metrics):\n",
    "        for case, marker in [(\"good\", \"o\"), (\"overfit\", \"x\")]:\n",
    "            sub = combined_df[combined_df[\"case_type\"] == case]\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "            ax.scatter(sub[metric], sub[\"test_accuracy\"], alpha=0.8, marker=marker, label=f\"{case} (test)\")\n",
    "        ax.set_xlabel(metric)\n",
    "        ax.set_ylabel(\"Test accuracy\")\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.set_title(f\"Test accuracy vs {metric}\")\n",
    "\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        fig.legend(handles, labels, loc=\"upper center\", ncol=4)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    combined_df.boxplot(column=[\"alpha\", \"ERG_gap\", \"num_traps\", \"accuracy_gap\"], by=\"case_type\", layout=(1, 4), figsize=(18, 4))\n",
    "    plt.suptitle(\"WW and gap metrics by case type\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "a_SNj9ibYHgA"
   },
   "id": "a_SNj9ibYHgA",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!head -200 $CHECKPOINT_AGGREGATED_CSV\n"
   ],
   "metadata": {
    "id": "6hc1QW9HAoQc"
   },
   "id": "6hc1QW9HAoQc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_9cTHchdNwhS"
   },
   "id": "_9cTHchdNwhS",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "XGBWW_Catalog_Random100_XGBoost_Accuracy.ipynb",
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "H100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}