{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ1fzZCeRp1J"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CalculatedContent/xgboost2ww/blob/main/notebooks/XGBWW_Catalog_Random100_XGBoost_Accuracy_WithOverfitCatalog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# What this notebook does\n",
    "\n",
    "This notebook benchmarks **XGBoost classification accuracy** across a random sample of catalog datasets, then intentionally trains **overfit variants per completed dataset** to make failure modes visible.\n",
    "\n",
    "## End-to-end workflow\n",
    "1. Load `dataset_catalog.csv` from Drive and keep classification tasks.\n",
    "2. Sample datasets with a fixed random seed and persist the selected list.\n",
    "3. Train one baseline (\u201cgood\u201d) model per sampled dataset.\n",
    "4. For each completed baseline dataset, train one model per selected overfit mode (`OVERFIT_MODES[:MAX_OVERFIT_CASES]`).\n",
    "5. Run `xgboost2ww` conversion + WeightWatcher metrics for both baseline and overfit runs.\n",
    "6. Save checkpoint files continuously so interrupted runs can resume.\n",
    "7. Write an aggregated output table containing both `case_type=\"good\"` and `case_type=\"overfit\"` rows.\n",
    "\n",
    "## Overfit behavior in this notebook\n",
    "- Overfit modes are configured by `OVERFIT_MODES` and capped by `MAX_OVERFIT_CASES` (default 6).\n",
    "- Modes are applied **per completed dataset** (not globally across only a few datasets).\n",
    "- Default modes:\n",
    "  1. `deep_trees`\n",
    "  2. `too_many_rounds`\n",
    "  3. `no_regularization`\n",
    "  4. `no_subsampling`\n",
    "  5. `tiny_trainset`\n",
    "  6. `leakage`\n",
    "\n",
    "## Runtime logging you will see\n",
    "- Baseline pass: dataset index progress (`[i/N]`), dataset UID, and final `train_accuracy` / `test_accuracy`.\n",
    "- Overfit pass: dataset index progress (`[dataset i/N]`), overfit mode index (`[overfit j/M]`), overfit mode name, and final `train_accuracy` / `test_accuracy`.\n",
    "- Skip/failure messages for datasets or modes that cannot be trained.\n",
    "\n",
    "## Expected outputs\n",
    "- `checkpoint_results.csv`: running status and per-run metrics.\n",
    "- `errors.csv`: failed/model_failed rows.\n",
    "- `results_per_dataset.csv`: completed baseline rows.\n",
    "- `results_summary_by_source.csv`: baseline source-level summary.\n",
    "- `checkpoint_results_good_plus_overfit.csv`: aggregated baseline + overfit rows.\n",
    "- `experiment_config.json`: experiment settings and counts.\n",
    "\n",
    "## Why include overfit cases?\n",
    "Including overfit runs alongside strong baseline models provides a direct comparison target for diagnostic plots and sanity checks on generalization behavior.\n",
    "\n"
   ],
   "id": "BZ1fzZCeRp1J"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XlqyD7nRp1K"
   },
   "source": [
    "# XGBWW catalog-driven random-per-source XGBoost benchmark + targeted overfit cases\n",
    "\n",
    "This notebook keeps the original catalog benchmark workflow, then adds intentionally overfit models per dataset (5\u20136 cases per dataset, based on `OVERFIT_MODES[:MAX_OVERFIT_CASES]`) and writes an aggregated checkpoint with both good and overfit results.\n"
   ],
   "id": "7XlqyD7nRp1K"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez6YDyX9Rp1L"
   },
   "source": [
    "## 1) Mount Google Drive and configure paths\n"
   ],
   "id": "Ez6YDyX9Rp1L"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DffIfgUGRp1L",
    "outputId": "8a4a0f70-708f-4023-8d50-39a51545aeb4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Catalog path: /content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv\n",
      "Experiment checkpoint: /content/drive/MyDrive/xgbwwdata/experiment_checkpoints/random100_xgboost_accuracy_plus_overfit_run01\n",
      "Restart mode: True\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ===== USER CONFIG =====\n",
    "CATALOG_CSV = Path(\"/content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv\")\n",
    "RANDOM_SEED = 42\n",
    "RANDOM_SAMPLE_SIZE = 100\n",
    "TEST_SIZE = 0.20\n",
    "EXPERIMENT_ROOT = Path(\"/content/drive/MyDrive/xgbwwdata/experiment_checkpoints\")\n",
    "DEFAULT_EXPERIMENT_BASENAME = \"random100_xgboost_accuracy_plus_overfit\"\n",
    "\n",
    "# Targeted overfit cases\n",
    "OVERFIT_MODES = [\n",
    "    \"deep_trees\",\n",
    "    \"too_many_rounds\",\n",
    "    \"no_regularization\",\n",
    "    \"no_subsampling\",\n",
    "    \"tiny_trainset\",\n",
    "    \"leakage\",\n",
    "]\n",
    "MAX_OVERFIT_CASES = 6\n",
    "TINY_TRAIN_FRAC = 0.05\n",
    "\n",
    "# Restart control\n",
    "RESTART_EXPERIMENT = True\n",
    "RETRY_FAILED_DATASETS = False  # Default: do not retry failed/model_failed datasets on restart.\n",
    "EXPERIMENT_NAME = \"random100_xgboost_accuracy_plus_overfit_run03\" # Required for restart.\n",
    "AUTO_INCREMENT_IF_NAME_MISSING = True\n",
    "# =======================\n",
    "\n",
    "\n",
    "def next_experiment_name(root: Path, base_name: str) -> str:\n",
    "    existing = [d.name for d in root.glob(f\"{base_name}_run*\") if d.is_dir()]\n",
    "    nums = []\n",
    "    for name in existing:\n",
    "        suffix = name.replace(f\"{base_name}_run\", \"\")\n",
    "        if suffix.isdigit():\n",
    "            nums.append(int(suffix))\n",
    "    n = (max(nums) + 1) if nums else 1\n",
    "    return f\"{base_name}_run{n:02d}\"\n",
    "\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "EXPERIMENT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if RESTART_EXPERIMENT:\n",
    "    if not EXPERIMENT_NAME:\n",
    "        raise ValueError(\"Set EXPERIMENT_NAME when RESTART_EXPERIMENT=True.\")\n",
    "    EXPERIMENT_ID = EXPERIMENT_NAME\n",
    "else:\n",
    "    if EXPERIMENT_NAME:\n",
    "        EXPERIMENT_ID = EXPERIMENT_NAME\n",
    "    elif AUTO_INCREMENT_IF_NAME_MISSING:\n",
    "        EXPERIMENT_ID = next_experiment_name(EXPERIMENT_ROOT, DEFAULT_EXPERIMENT_BASENAME)\n",
    "    else:\n",
    "        EXPERIMENT_ID = DEFAULT_EXPERIMENT_BASENAME\n",
    "\n",
    "CHECKPOINT_DIR = EXPERIMENT_ROOT / EXPERIMENT_ID\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_RESULTS_CSV = CHECKPOINT_DIR / \"checkpoint_results.csv\"\n",
    "CHECKPOINT_ERRORS_CSV = CHECKPOINT_DIR / \"errors.csv\"\n",
    "CHECKPOINT_AGGREGATED_CSV = CHECKPOINT_DIR / \"checkpoint_results_good_plus_overfit.csv\"\n",
    "OVERFIT_RESULTS_CSV = CHECKPOINT_DIR / \"overfit_results.csv\"\n",
    "SELECTED_DATASETS_CSV = CHECKPOINT_DIR / \"selected_datasets.csv\"\n",
    "EXPERIMENT_CONFIG_JSON = CHECKPOINT_DIR / \"experiment_config.json\"\n",
    "\n",
    "print(\"Catalog path:\", CATALOG_CSV)\n",
    "print(\"Experiment checkpoint:\", CHECKPOINT_DIR)\n",
    "print(\"Restart mode:\", RESTART_EXPERIMENT)\n",
    "print(\"Overfit results CSV:\", OVERFIT_RESULTS_CSV)\n",
    "\n",
    "\n"
   ],
   "id": "DffIfgUGRp1L"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How far did we get before ?\n"
   ],
   "metadata": {
    "id": "tLSqXOADsgI_"
   },
   "id": "tLSqXOADsgI_"
  },
  {
   "cell_type": "code",
   "source": [
    "# Progress snapshot from Google Drive checkpoint.\n",
    "# Restart behavior:\n",
    "# - status == \"completed\" is always skipped\n",
    "# - failed/model_failed are retried only when RETRY_FAILED_DATASETS=True\n",
    "# - pending/missing are always run\n",
    "selected_df = pd.read_csv(SELECTED_DATASETS_CSV) if SELECTED_DATASETS_CSV.exists() else None\n",
    "checkpoint_df = pd.read_csv(CHECKPOINT_RESULTS_CSV) if CHECKPOINT_RESULTS_CSV.exists() else pd.DataFrame()\n",
    "\n",
    "if selected_df is not None and \"dataset_uid\" in selected_df.columns:\n",
    "    target_uids = selected_df[\"dataset_uid\"].astype(str).tolist()\n",
    "else:\n",
    "    target_uids = checkpoint_df.get(\"dataset_uid\", pd.Series(dtype=str)).astype(str).tolist()\n",
    "    if not target_uids:\n",
    "        target_uids = [None] * RANDOM_SAMPLE_SIZE\n",
    "\n",
    "status_by_uid = {}\n",
    "if not checkpoint_df.empty and \"dataset_uid\" in checkpoint_df.columns:\n",
    "    checkpoint_df = checkpoint_df.drop_duplicates(subset=[\"dataset_uid\"], keep=\"last\")\n",
    "    if \"status\" in checkpoint_df.columns:\n",
    "        status_by_uid = dict(zip(checkpoint_df[\"dataset_uid\"].astype(str), checkpoint_df[\"status\"].astype(str)))\n",
    "\n",
    "completed_models = 0\n",
    "remaining_models = 0\n",
    "next_uid = None\n",
    "next_index = None\n",
    "\n",
    "for idx, uid in enumerate(target_uids, start=1):\n",
    "    uid_key = str(uid) if uid is not None else None\n",
    "    status = status_by_uid.get(uid_key, \"missing\")\n",
    "\n",
    "    if status == \"completed\":\n",
    "        completed_models += 1\n",
    "        should_run = False\n",
    "    elif status in {\"failed\", \"model_failed\"}:\n",
    "        should_run = RETRY_FAILED_DATASETS\n",
    "    else:\n",
    "        should_run = True\n",
    "\n",
    "    if should_run:\n",
    "        remaining_models += 1\n",
    "        if next_uid is None:\n",
    "            next_uid = uid_key\n",
    "            next_index = idx\n",
    "\n",
    "failed_statuses = {\"failed\", \"model_failed\"}\n",
    "failed_models = sum(1 for uid in target_uids if status_by_uid.get(str(uid), \"missing\") in failed_statuses)\n",
    "\n",
    "print(f\"Total selected models: {len(target_uids)}\")\n",
    "print(f\"Completed models: {completed_models}\")\n",
    "print(f\"Failed models currently on checkpoint: {failed_models}\")\n",
    "print(f\"Retry failed datasets on restart: {RETRY_FAILED_DATASETS}\")\n",
    "print(f\"Remaining models to run on restart: {remaining_models}\")\n",
    "if next_uid is not None:\n",
    "    print(f\"Restart will resume at dataset #{next_index}: {next_uid}\")\n",
    "else:\n",
    "    print(\"Checkpoint is fully completed. No more models remain.\")"
   ],
   "metadata": {
    "id": "Z34Aur-qsgQQ",
    "outputId": "3471e7f9-afde-4a24-bf41-25c62cfe2bc1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "Z34Aur-qsgQQ",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total selected models: 500\n",
      "Completed models: 0\n",
      "Failed models currently on checkpoint: 0\n",
      "Retry failed datasets on restart: False\n",
      "Remaining models to run on restart: 500\n",
      "Restart will resume at dataset #1: openml:75\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT9KuFpNRp1L"
   },
   "source": [
    "## 2) Install dependencies\n",
    "\n",
    "Use the same repository-install flow as the other Colab notebooks (no `pip install xgbwwdata`).\n"
   ],
   "id": "aT9KuFpNRp1L"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4cFzOrOvRp1M",
    "outputId": "17c8265f-2459-47a9-db88-0a2a39c42441",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into '/content/repo_xgbwwdata'...\n",
      "remote: Enumerating objects: 142, done.\u001b[K\n",
      "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
      "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
      "remote: Total 142 (delta 11), reused 0 (delta 0), pack-reused 100 (from 2)\u001b[K\n",
      "Receiving objects: 100% (142/142), 233.90 KiB | 1.96 MiB/s, done.\n",
      "Resolving deltas: 100% (48/48), done.\n",
      "+ /usr/bin/python3 -m pip install -U pip setuptools wheel\n",
      "+ /usr/bin/python3 -m pip install -r /content/repo_xgbwwdata/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Install xgbwwdata from a fresh clone using the repository installer script\n",
    "!rm -rf /content/repo_xgbwwdata\n",
    "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\n",
    "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\n",
    "\n",
    "# Notebook-specific dependencies\n",
    "%pip install -q openml pmlb keel-ds xgboost scikit-learn xgboost2ww weightwatcher\n"
   ],
   "id": "4cFzOrOvRp1M"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM2nQyMGRp1M"
   },
   "source": [
    "## 3) Imports\n"
   ],
   "id": "FM2nQyMGRp1M"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p3LUiyt9Rp1M"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import weightwatcher as ww\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgbwwdata import Filters, load_dataset\n",
    "from xgboost2ww import convert\n"
   ],
   "id": "p3LUiyt9Rp1M"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxUyvl08Rp1M"
   },
   "source": [
    "## 4) Load catalog and pick 100 random dataset UIDs (checkpoint-aware)\n"
   ],
   "id": "sxUyvl08Rp1M"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R-esy_yzRp1M"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not CATALOG_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Catalog not found: {CATALOG_CSV}. Run XGBWW_Dataset_Catalog_Checkpoint.ipynb first.\")\n",
    "\n",
    "df_catalog = pd.read_csv(CATALOG_CSV)\n",
    "print(\"Catalog shape:\", df_catalog.shape)\n",
    "\n",
    "required_cols = {\"dataset_uid\", \"source\", \"task_type\"}\n",
    "missing = required_cols - set(df_catalog.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Catalog is missing required columns: {missing}\")\n",
    "\n",
    "# Accuracy is for classification; keep classification-like tasks\n",
    "df_cls = df_catalog[df_catalog[\"task_type\"].astype(str).str.contains(\"classification\", case=False, na=False)].copy()\n",
    "if df_cls.empty:\n",
    "    raise ValueError(\"No classification datasets found in catalog.\")\n",
    "\n",
    "# Select 100 random dataset UIDs up front for this experiment.\n",
    "n_select = min(RANDOM_SAMPLE_SIZE, len(df_cls))\n",
    "df_pick = df_cls.sample(n=n_select, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Selected datasets:\", len(df_pick))\n",
    "display(df_pick[[\"source\", \"dataset_uid\", \"name\", \"task_type\"]].sort_values([\"source\", \"dataset_uid\"]))\n",
    "\n",
    "# Initialize or reload checkpoint table\n",
    "if RESTART_EXPERIMENT and CHECKPOINT_RESULTS_CSV.exists():\n",
    "    checkpoint_df = pd.read_csv(CHECKPOINT_RESULTS_CSV)\n",
    "    print(f\"Loaded existing checkpoint rows: {len(checkpoint_df)}\")\n",
    "else:\n",
    "    checkpoint_df = df_pick.copy()\n",
    "    checkpoint_df[\"status\"] = \"pending\"\n",
    "    checkpoint_df[\"error_message\"] = \"\"\n",
    "\n",
    "    # model / hyperparameter fields (blank at initialization)\n",
    "    checkpoint_df[\"xgboost_params\"] = \"\"\n",
    "    checkpoint_df[\"rounds\"] = np.nan\n",
    "    checkpoint_df[\"n_classes\"] = np.nan\n",
    "    checkpoint_df[\"train_size\"] = np.nan\n",
    "    checkpoint_df[\"test_size_rows\"] = np.nan\n",
    "\n",
    "    # train/test accuracy fields\n",
    "    checkpoint_df[\"train_accuracy\"] = np.nan\n",
    "    checkpoint_df[\"test_accuracy\"] = np.nan\n",
    "\n",
    "    # weightwatcher metrics\n",
    "    checkpoint_df[\"alpha\"] = np.nan\n",
    "    checkpoint_df[\"ERG_gap\"] = np.nan\n",
    "    checkpoint_df[\"num_traps\"] = np.nan\n",
    "\n",
    "    # metadata from actual loaded data\n",
    "    checkpoint_df[\"dataset_name\"] = checkpoint_df.get(\"name\", \"\")\n",
    "    checkpoint_df[\"dataset_openml_id\"] = checkpoint_df.get(\"openml_data_id\", np.nan)\n",
    "    checkpoint_df[\"n_rows\"] = np.nan\n",
    "    checkpoint_df[\"n_features\"] = np.nan\n",
    "    checkpoint_df[\"test_size\"] = TEST_SIZE\n",
    "\n",
    "# Ensure the selected datasets are saved for restart reproducibility\n",
    "if not SELECTED_DATASETS_CSV.exists():\n",
    "    df_pick.to_csv(SELECTED_DATASETS_CSV, index=False)\n",
    "\n",
    "# If restarting, trust existing selected datasets file when present\n",
    "if RESTART_EXPERIMENT and SELECTED_DATASETS_CSV.exists():\n",
    "    df_pick = pd.read_csv(SELECTED_DATASETS_CSV)\n",
    "\n",
    "# Save initial checkpoint immediately\n",
    "checkpoint_df.to_csv(CHECKPOINT_RESULTS_CSV, index=False)\n",
    "print(\"Checkpoint file:\", CHECKPOINT_RESULTS_CSV)\n"
   ],
   "id": "R-esy_yzRp1M"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsD9mQFdRp1M"
   },
   "source": [
    "## 5) Train one XGBoost model per sampled dataset, run xgboost2ww + WeightWatcher, and report metrics\n"
   ],
   "id": "tsD9mQFdRp1M"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uhd4vPK7Rp1N"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "filters = Filters(\n",
    "    min_rows=200,\n",
    "    max_rows=60000,\n",
    "    max_features=50000,\n",
    "    max_dense_elements=int(2e8),\n",
    ")\n",
    "\n",
    "\n",
    "class ModelTrainingFailed(RuntimeError):\n",
    "    \"\"\"Raised when XGBoost model fitting fails for a dataset.\"\"\"\n",
    "\n",
    "\n",
    "def detect_xgb_compute_params():\n",
    "    \"\"\"Prefer CUDA in Colab when available; gracefully fall back to CPU.\"\"\"\n",
    "    gpu_params = {\"tree_method\": \"hist\", \"device\": \"cuda\"}\n",
    "    cpu_params = {\"tree_method\": \"hist\"}\n",
    "\n",
    "    X_probe = np.array([[0.0], [1.0], [2.0], [3.0]], dtype=np.float32)\n",
    "    y_probe = np.array([0, 0, 1, 1], dtype=np.float32)\n",
    "    dprobe = xgb.DMatrix(X_probe, label=y_probe)\n",
    "\n",
    "    try:\n",
    "        xgb.train(\n",
    "            params={\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"eval_metric\": \"logloss\",\n",
    "                **gpu_params,\n",
    "            },\n",
    "            dtrain=dprobe,\n",
    "            num_boost_round=1,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        return gpu_params, \"gpu\"\n",
    "    except Exception:\n",
    "        return cpu_params, \"cpu\"\n",
    "\n",
    "\n",
    "def apply_overfit_mode(params: dict, mode: str, seed: int):\n",
    "    p = dict(params)\n",
    "    rng = np.random.default_rng(seed + 999)\n",
    "\n",
    "    if mode == \"deep_trees\":\n",
    "        p[\"max_depth\"] = int(rng.integers(12, 19))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.2, 0.45))\n",
    "        p[\"min_child_weight\"] = float(rng.uniform(1.0, 4.0))\n",
    "        p[\"reg_lambda\"] = 0.0\n",
    "        p[\"reg_alpha\"] = 0.0\n",
    "    elif mode == \"too_many_rounds\":\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.25, 0.5))\n",
    "        p[\"max_depth\"] = int(rng.integers(6, 12))\n",
    "        p[\"subsample\"] = 1.0\n",
    "        p[\"colsample_bytree\"] = 1.0\n",
    "    elif mode == \"no_regularization\":\n",
    "        p[\"reg_lambda\"] = 0.0\n",
    "        p[\"reg_alpha\"] = 0.0\n",
    "        p[\"gamma\"] = 0.0\n",
    "        p[\"max_depth\"] = int(rng.integers(7, 13))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.2, 0.45))\n",
    "    elif mode == \"no_subsampling\":\n",
    "        p[\"subsample\"] = 1.0\n",
    "        p[\"colsample_bytree\"] = 1.0\n",
    "        p[\"max_depth\"] = int(rng.integers(6, 11))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.2, 0.45))\n",
    "    elif mode == \"tiny_trainset\":\n",
    "        p[\"max_depth\"] = int(rng.integers(9, 15))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.25, 0.5))\n",
    "        p[\"subsample\"] = 1.0\n",
    "        p[\"colsample_bytree\"] = 1.0\n",
    "    elif mode == \"leakage\":\n",
    "        p[\"max_depth\"] = int(rng.integers(5, 10))\n",
    "        p[\"learning_rate\"] = float(rng.uniform(0.2, 0.45))\n",
    "        p[\"subsample\"] = 1.0\n",
    "        p[\"colsample_bytree\"] = 1.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown overfit mode: {mode}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def training_schedule(case_type: str, overfit_mode: str | None):\n",
    "    if case_type == \"overfit\" and overfit_mode == \"too_many_rounds\":\n",
    "        return 7000, None\n",
    "    if case_type == \"overfit\":\n",
    "        return 3000, None\n",
    "    return 1200, None\n",
    "\n",
    "\n",
    "XGB_COMPUTE_PARAMS, XGB_COMPUTE_BACKEND = detect_xgb_compute_params()\n",
    "print(f\"XGBoost compute backend detected: {XGB_COMPUTE_BACKEND} | params={XGB_COMPUTE_PARAMS}\")\n",
    "\n",
    "\n",
    "def fit_and_score(row_data: dict, case_type: str = \"good\", overfit_mode: str | None = None, seed_offset: int = 0):\n",
    "    dataset_uid = row_data[\"dataset_uid\"]\n",
    "    source = row_data[\"source\"]\n",
    "    local_seed = RANDOM_SEED + seed_offset\n",
    "\n",
    "    X, y, meta = load_dataset(dataset_uid, filters=filters)\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    classes, y_enc = np.unique(y, return_inverse=True)\n",
    "    n_classes = len(classes)\n",
    "    if n_classes < 2:\n",
    "        raise ValueError(f\"Dataset {dataset_uid} has <2 classes after loading.\")\n",
    "\n",
    "    stratify = y_enc if n_classes > 1 else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_enc, test_size=TEST_SIZE, random_state=local_seed, stratify=stratify\n",
    "    )\n",
    "\n",
    "    if case_type == \"overfit\" and overfit_mode == \"tiny_trainset\":\n",
    "        sub_idx, _ = train_test_split(\n",
    "            np.arange(len(y_train)),\n",
    "            test_size=(1.0 - TINY_TRAIN_FRAC),\n",
    "            random_state=local_seed,\n",
    "            stratify=y_train,\n",
    "        )\n",
    "        X_train, y_train = X_train[sub_idx], y_train[sub_idx]\n",
    "\n",
    "    if case_type == \"overfit\" and overfit_mode == \"leakage\":\n",
    "        rng = np.random.default_rng(local_seed)\n",
    "        Xtr_dense = X_train.toarray() if hasattr(X_train, \"toarray\") else np.asarray(X_train)\n",
    "        Xte_dense = X_test.toarray() if hasattr(X_test, \"toarray\") else np.asarray(X_test)\n",
    "        leak_tr = (y_train + 0.05 * rng.standard_normal(len(y_train))).astype(np.float32).reshape(-1, 1)\n",
    "        leak_te = (y_test + 0.05 * rng.standard_normal(len(y_test))).astype(np.float32).reshape(-1, 1)\n",
    "        X_train = np.hstack([Xtr_dense.astype(np.float32), leak_tr])\n",
    "        X_test = np.hstack([Xte_dense.astype(np.float32), leak_te])\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    try:\n",
    "        if n_classes == 2:\n",
    "            params = {\n",
    "                \"objective\": \"binary:logistic\",\n",
    "                \"eval_metric\": \"logloss\",\n",
    "                **XGB_COMPUTE_PARAMS,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"max_depth\": 6,\n",
    "                \"subsample\": 0.85,\n",
    "                \"colsample_bytree\": 0.85,\n",
    "                \"min_child_weight\": 2.0,\n",
    "                \"reg_lambda\": 2.0,\n",
    "                \"reg_alpha\": 0.2,\n",
    "                \"gamma\": 0.1,\n",
    "                \"seed\": local_seed,\n",
    "            }\n",
    "            if case_type == \"overfit\":\n",
    "                params = apply_overfit_mode(params, overfit_mode, local_seed)\n",
    "            num_boost_round, early_stop = training_schedule(case_type, overfit_mode)\n",
    "            if early_stop is None:\n",
    "                cv = xgb.cv(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    early_stopping_rounds=50,\n",
    "                    seed=local_seed,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = len(cv)\n",
    "                model = xgb.train(params=params, dtrain=dtrain, num_boost_round=rounds, verbose_eval=False)\n",
    "            else:\n",
    "                model = xgb.train(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "                    early_stopping_rounds=early_stop,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = int(model.best_iteration + 1)\n",
    "\n",
    "            yhat_tr = (model.predict(dtrain) >= 0.5).astype(int)\n",
    "            yhat_te = (model.predict(dtest) >= 0.5).astype(int)\n",
    "        else:\n",
    "            params = {\n",
    "                \"objective\": \"multi:softprob\",\n",
    "                \"num_class\": n_classes,\n",
    "                \"eval_metric\": \"mlogloss\",\n",
    "                **XGB_COMPUTE_PARAMS,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"max_depth\": 7,\n",
    "                \"subsample\": 0.9,\n",
    "                \"colsample_bytree\": 0.9,\n",
    "                \"min_child_weight\": 1.0,\n",
    "                \"reg_lambda\": 1.0,\n",
    "                \"reg_alpha\": 0.1,\n",
    "                \"gamma\": 0.0,\n",
    "                \"seed\": local_seed,\n",
    "            }\n",
    "            if case_type == \"overfit\":\n",
    "                params = apply_overfit_mode(params, overfit_mode, local_seed)\n",
    "            num_boost_round, early_stop = training_schedule(case_type, overfit_mode)\n",
    "            if early_stop is None:\n",
    "                cv = xgb.cv(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    early_stopping_rounds=60,\n",
    "                    seed=local_seed,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = len(cv)\n",
    "                model = xgb.train(params=params, dtrain=dtrain, num_boost_round=rounds, verbose_eval=False)\n",
    "            else:\n",
    "                model = xgb.train(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "                    early_stopping_rounds=early_stop,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "                rounds = int(model.best_iteration + 1)\n",
    "\n",
    "            yhat_tr = np.argmax(model.predict(dtrain), axis=1)\n",
    "            yhat_te = np.argmax(model.predict(dtest), axis=1)\n",
    "    except Exception as e:\n",
    "        raise ModelTrainingFailed(f\"Model training failed for dataset {dataset_uid}: {e}\") from e\n",
    "\n",
    "    ww_layer = convert(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        W=\"W7\",\n",
    "        nfolds=5,\n",
    "        t_points=160,\n",
    "        random_state=local_seed,\n",
    "        train_params=params,\n",
    "        num_boost_round=rounds,\n",
    "        multiclass=\"avg\" if n_classes > 2 else \"error\",\n",
    "        return_type=\"torch\",\n",
    "    )\n",
    "    watcher = ww.WeightWatcher(model=ww_layer)\n",
    "    details_df = watcher.analyze(ERG=True, randomize=True, plot=False)\n",
    "\n",
    "    alpha = float(details_df[\"alpha\"].iloc[0]) if \"alpha\" in details_df else np.nan\n",
    "    erg_gap = float(details_df[\"ERG_gap\"].iloc[0]) if \"ERG_gap\" in details_df else np.nan\n",
    "    num_traps = float(details_df[\"num_traps\"].iloc[0]) if \"num_traps\" in details_df else np.nan\n",
    "\n",
    "    result = {\n",
    "        \"source\": source,\n",
    "        \"dataset_uid\": dataset_uid,\n",
    "        \"dataset_name\": row_data.get(\"name\", meta.get(\"name\")),\n",
    "        \"task_type\": row_data.get(\"task_type\"),\n",
    "        \"dataset_openml_id\": row_data.get(\"openml_data_id\"),\n",
    "        \"n_rows\": int(meta.get(\"n_rows\", len(y))),\n",
    "        \"n_features\": int(meta.get(\"n_features\", X.shape[1] if hasattr(X, \"shape\") else -1)),\n",
    "        \"n_classes\": int(n_classes),\n",
    "        \"test_size\": float(TEST_SIZE),\n",
    "        \"train_size\": int(len(y_train)),\n",
    "        \"test_size_rows\": int(len(y_test)),\n",
    "        \"rounds\": int(rounds),\n",
    "        \"train_accuracy\": float(accuracy_score(y_train, yhat_tr)),\n",
    "        \"test_accuracy\": float(accuracy_score(y_test, yhat_te)),\n",
    "        \"accuracy_gap\": float(accuracy_score(y_train, yhat_tr) - accuracy_score(y_test, yhat_te)),\n",
    "        \"alpha\": alpha,\n",
    "        \"ERG_gap\": erg_gap,\n",
    "        \"num_traps\": num_traps,\n",
    "        \"case_type\": case_type,\n",
    "        \"overfit_mode\": overfit_mode if overfit_mode else \"none\",\n",
    "        \"seed_used\": int(local_seed),\n",
    "        \"xgboost_params\": json.dumps(params, sort_keys=True),\n",
    "        \"status\": \"completed\",\n",
    "        \"error_message\": \"\",\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_checkpoint_row(uid: str, updates: dict):\n",
    "    global checkpoint_df\n",
    "    mask = checkpoint_df[\"dataset_uid\"] == uid\n",
    "    if not mask.any():\n",
    "        return\n",
    "    for k, v in updates.items():\n",
    "        if k not in checkpoint_df.columns:\n",
    "            checkpoint_df[k] = np.nan\n",
    "        checkpoint_df.loc[mask, k] = v\n",
    "    checkpoint_df.to_csv(CHECKPOINT_RESULTS_CSV, index=False)\n",
    "\n",
    "\n",
    "total_models = len(df_pick)\n",
    "\n",
    "for model_idx, row in enumerate(df_pick.itertuples(index=False), start=1):\n",
    "    row_data = row._asdict()\n",
    "    uid = row_data[\"dataset_uid\"]\n",
    "    progress = f\"[{model_idx}/{total_models}]\"\n",
    "\n",
    "    existing = checkpoint_df.loc[checkpoint_df[\"dataset_uid\"] == uid, \"status\"]\n",
    "    if not existing.empty:\n",
    "        existing_status = str(existing.iloc[0])\n",
    "        if existing_status == \"completed\":\n",
    "            print(f\"{progress} Skipping completed dataset: {uid}\")\n",
    "            continue\n",
    "        if existing_status in {\"failed\", \"model_failed\"} and not RETRY_FAILED_DATASETS:\n",
    "            print(f\"{progress} Skipping failed dataset (retry disabled): {uid}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"{progress} Training + WW analysis: {uid} | case_type=good\")\n",
    "    try:\n",
    "        result = fit_and_score(row_data, case_type=\"good\", overfit_mode=None, seed_offset=0)\n",
    "        update_checkpoint_row(uid, result)\n",
    "        print(\n",
    "            f\"{progress} Completed: {uid} | train_accuracy={result['train_accuracy']:.4f} \"\n",
    "            f\"| test_accuracy={result['test_accuracy']:.4f}\"\n",
    "        )\n",
    "    except ModelTrainingFailed as e:\n",
    "        err_msg = str(e)\n",
    "        update_checkpoint_row(uid, {\"status\": \"model_failed\", \"error_message\": err_msg})\n",
    "        print(f\"{progress} Marked model_failed for {uid}: {err_msg}\")\n",
    "    except Exception as e:\n",
    "        err_msg = str(e)\n",
    "        update_checkpoint_row(uid, {\"status\": \"failed\", \"error_message\": err_msg})\n",
    "        print(f\"{progress} Skipped {uid}: {err_msg}\")\n",
    "\n",
    "results_df = checkpoint_df[checkpoint_df[\"status\"] == \"completed\"].copy()\n",
    "errors_df = checkpoint_df[checkpoint_df[\"status\"] != \"completed\"][[\"source\", \"dataset_uid\", \"status\", \"error_message\"]].rename(columns={\"error_message\": \"error\"}).copy()\n",
    "\n",
    "if not errors_df.empty:\n",
    "    errors_df.to_csv(CHECKPOINT_ERRORS_CSV, index=False)\n",
    "\n",
    "print(\"Completed:\", len(results_df), \"datasets\")\n",
    "print(\"Failed:\", len(errors_df), \"datasets\")\n",
    "\n",
    "display(results_df.sort_values([\"source\", \"test_accuracy\"], ascending=[True, False]))\n"
   ],
   "id": "uhd4vPK7Rp1N"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qma8FIOpRp1N"
   },
   "source": [
    "## 6) Summary tables (accuracy + WeightWatcher metrics)\n"
   ],
   "id": "qma8FIOpRp1N"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lfZrM9vORp1N"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "if results_df.empty:\n",
    "    print(\"No successful trainings.\")\n",
    "else:\n",
    "    metric_cols = [\"train_accuracy\", \"test_accuracy\", \"accuracy_gap\", \"alpha\", \"ERG_gap\", \"num_traps\"]\n",
    "    summary = (\n",
    "        results_df.groupby(\"source\", as_index=False)[metric_cols]\n",
    "        .agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "    )\n",
    "    summary.columns = [\"source\"] + [f\"{a}_{b}\" for a, b in summary.columns.tolist()[1:]]\n",
    "\n",
    "    print(\"Per-dataset results (good models):\")\n",
    "    display(results_df.sort_values([\"source\", \"test_accuracy\"], ascending=[True, False]))\n",
    "\n",
    "    print(\"Per-source summary (good models):\")\n",
    "    display(summary.sort_values(\"test_accuracy_mean\", ascending=False))\n",
    "\n",
    "    experiment_config = {\n",
    "        \"experiment_id\": EXPERIMENT_ID,\n",
    "        \"experiment_name\": EXPERIMENT_ID,\n",
    "        \"restart_experiment\": RESTART_EXPERIMENT,\n",
    "        \"catalog_csv\": str(CATALOG_CSV),\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"random_sample_size\": RANDOM_SAMPLE_SIZE,\n",
    "        \"test_size\": TEST_SIZE,\n",
    "        \"overfit_modes\": OVERFIT_MODES,\n",
    "        \"max_overfit_cases\": MAX_OVERFIT_CASES,\n",
    "        \"selected_dataset_count\": int(len(df_pick)),\n",
    "        \"successful_dataset_count\": int(len(results_df)),\n",
    "        \"failed_dataset_count\": int(len(errors_df)),\n",
    "    }\n",
    "\n",
    "    results_path = CHECKPOINT_DIR / \"results_per_dataset.csv\"\n",
    "    summary_path = CHECKPOINT_DIR / \"results_summary_by_source.csv\"\n",
    "\n",
    "    checkpoint_df.to_csv(CHECKPOINT_RESULTS_CSV, index=False)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    errors_df.to_csv(CHECKPOINT_ERRORS_CSV, index=False)\n",
    "    with open(EXPERIMENT_CONFIG_JSON, \"w\") as f:\n",
    "        json.dump(experiment_config, f, indent=2)\n",
    "\n",
    "    print(\"Saved checkpoint files:\")\n",
    "    print(\" -\", CHECKPOINT_RESULTS_CSV)\n",
    "    print(\" -\", results_path)\n",
    "    print(\" -\", summary_path)\n",
    "    print(\" -\", CHECKPOINT_ERRORS_CSV)\n",
    "    print(\" -\", EXPERIMENT_CONFIG_JSON)\n"
   ],
   "id": "lfZrM9vORp1N"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZVL3LcURp1N"
   },
   "source": [
    "## 7) Accuracy comparison plots vs WeightWatcher metrics\n"
   ],
   "id": "VZVL3LcURp1N"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVlB0ke9Rp1N"
   },
   "outputs": [],
   "source": [
    "if results_df.empty:\n",
    "    print(\"No successful trainings to plot.\")\n",
    "else:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plot_df = results_df.sort_values([\"source\", \"dataset_uid\"]).copy()\n",
    "\n",
    "    metrics = [\"alpha\", \"ERG_gap\", \"num_traps\"]\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 5), squeeze=False)\n",
    "\n",
    "    for ax, metric in zip(axes[0], metrics):\n",
    "        ax.scatter(plot_df[metric], plot_df[\"train_accuracy\"], label=\"Train accuracy\", alpha=0.8)\n",
    "        ax.scatter(plot_df[metric], plot_df[\"test_accuracy\"], label=\"Test accuracy\", alpha=0.8)\n",
    "        ax.set_xlabel(metric)\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.set_ylim(0.4, 1.05)\n",
    "        if metric == \"alpha\":\n",
    "            ax.set_xlim(1.5, 6)\n",
    "        ax.set_title(f\"Good models: Accuracy vs {metric}\")\n",
    "        ax.grid(alpha=0.2)\n",
    "\n",
    "    axes[0, 0].legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "wVlB0ke9Rp1N"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8) Train targeted overfit cases for each completed dataset and build aggregated checkpoint\n"
   ],
   "metadata": {
    "id": "qDmk6e5iAkpL"
   },
   "id": "qDmk6e5iAkpL"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No completed good models. Cannot build overfit comparison set.\")\n",
    "    combined_df = results_df.copy()\n",
    "    overfit_df = pd.DataFrame()\n",
    "else:\n",
    "    good_df = results_df.copy()\n",
    "    good_df[\"case_type\"] = good_df.get(\"case_type\", \"good\")\n",
    "    good_df[\"overfit_mode\"] = good_df.get(\"overfit_mode\", \"none\")\n",
    "    if \"accuracy_gap\" not in good_df.columns:\n",
    "        good_df[\"accuracy_gap\"] = good_df[\"train_accuracy\"] - good_df[\"test_accuracy\"]\n",
    "\n",
    "    # Restart-aware overfit checkpointing:\n",
    "    # - load previously saved overfit rows from CHECKPOINT_AGGREGATED_CSV\n",
    "    # - skip already completed (dataset_uid, overfit_mode) pairs\n",
    "    # - persist after every overfit case so long runs can resume exactly\n",
    "    overfit_cols = [\n",
    "        \"case_type\", \"dataset_uid\", \"overfit_mode\", \"status\", \"error_message\",\n",
    "        \"source\", \"train_accuracy\", \"test_accuracy\", \"accuracy_gap\", \"alpha\", \"ERG_gap\", \"num_traps\"\n",
    "    ]\n",
    "    existing_overfit_df = pd.DataFrame(columns=overfit_cols)\n",
    "    if CHECKPOINT_AGGREGATED_CSV.exists():\n",
    "        existing_combined_df = pd.read_csv(CHECKPOINT_AGGREGATED_CSV)\n",
    "        if \"case_type\" in existing_combined_df.columns:\n",
    "            existing_overfit_df = existing_combined_df[\n",
    "                existing_combined_df[\"case_type\"].astype(str) == \"overfit\"\n",
    "            ].copy()\n",
    "            if \"status\" not in existing_overfit_df.columns:\n",
    "                existing_overfit_df[\"status\"] = \"completed\"\n",
    "            if \"error_message\" not in existing_overfit_df.columns:\n",
    "                existing_overfit_df[\"error_message\"] = \"\"\n",
    "\n",
    "    completed_pairs = set()\n",
    "    failed_pairs = set()\n",
    "    if not existing_overfit_df.empty:\n",
    "        existing_overfit_df[\"dataset_uid\"] = existing_overfit_df[\"dataset_uid\"].astype(str)\n",
    "        existing_overfit_df[\"overfit_mode\"] = existing_overfit_df[\"overfit_mode\"].astype(str)\n",
    "        completed_pairs = {\n",
    "            (uid, mode)\n",
    "            for uid, mode, status in zip(\n",
    "                existing_overfit_df[\"dataset_uid\"],\n",
    "                existing_overfit_df[\"overfit_mode\"],\n",
    "                existing_overfit_df[\"status\"].astype(str),\n",
    "            )\n",
    "            if status == \"completed\"\n",
    "        }\n",
    "        failed_pairs = {\n",
    "            (uid, mode)\n",
    "            for uid, mode, status in zip(\n",
    "                existing_overfit_df[\"dataset_uid\"],\n",
    "                existing_overfit_df[\"overfit_mode\"],\n",
    "                existing_overfit_df[\"status\"].astype(str),\n",
    "            )\n",
    "            if status == \"failed\"\n",
    "        }\n",
    "\n",
    "    per_dataset_modes = OVERFIT_MODES[:MAX_OVERFIT_CASES]\n",
    "    expected_total = len(good_df) * len(per_dataset_modes)\n",
    "    print(\n",
    "        f\"Creating up to {len(per_dataset_modes)} overfit cases per completed dataset \"\n",
    "        f\"using modes: {per_dataset_modes}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Existing overfit checkpoint rows: {len(existing_overfit_df)} \"\n",
    "        f\"| completed pairs: {len(completed_pairs)} | failed pairs: {len(failed_pairs)}\"\n",
    "    )\n",
    "\n",
    "    # Deduplicate by pair, preferring the latest row if present in existing checkpoint.\n",
    "    overfit_records = {}\n",
    "    if not existing_overfit_df.empty:\n",
    "        for _, r in existing_overfit_df.iterrows():\n",
    "            key = (str(r.get(\"dataset_uid\")), str(r.get(\"overfit_mode\")))\n",
    "            overfit_records[key] = r.to_dict()\n",
    "\n",
    "    total_good = len(good_df)\n",
    "    for ds_idx, (_, row) in enumerate(good_df.iterrows(), start=1):\n",
    "        row_data = row.to_dict()\n",
    "        uid = str(row_data[\"dataset_uid\"])\n",
    "        print(f\"[dataset {ds_idx}/{total_good}] dataset={uid} | case_type=overfit\")\n",
    "\n",
    "        for mode_idx, mode in enumerate(per_dataset_modes, start=1):\n",
    "            pair = (uid, str(mode))\n",
    "            print(f\"  [overfit {mode_idx}/{len(per_dataset_modes)}] mode={mode}\")\n",
    "\n",
    "            if pair in completed_pairs:\n",
    "                print(f\"    SKIP completed overfit checkpoint pair={pair}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                overfit_result = fit_and_score(\n",
    "                    row_data,\n",
    "                    case_type=\"overfit\",\n",
    "                    overfit_mode=mode,\n",
    "                    seed_offset=1000 + (ds_idx * 100) + mode_idx,\n",
    "                )\n",
    "                overfit_result[\"status\"] = \"completed\"\n",
    "                overfit_result[\"error_message\"] = \"\"\n",
    "                print(\n",
    "                    f\"    [dataset {ds_idx}/{total_good}] overfit_mode={mode} \"\n",
    "                    f\"| train_accuracy={overfit_result['train_accuracy']:.4f} \"\n",
    "                    f\"| test_accuracy={overfit_result['test_accuracy']:.4f}\"\n",
    "                )\n",
    "                overfit_records[pair] = overfit_result\n",
    "                completed_pairs.add(pair)\n",
    "                failed_pairs.discard(pair)\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                print(f\"    FAILED overfit case dataset={uid} mode={mode}: {err}\")\n",
    "                fail_row = {\n",
    "                    \"dataset_uid\": uid,\n",
    "                    \"source\": row_data.get(\"source\", \"unknown\"),\n",
    "                    \"case_type\": \"overfit\",\n",
    "                    \"overfit_mode\": mode,\n",
    "                    \"status\": \"failed\",\n",
    "                    \"error_message\": err,\n",
    "                    \"train_accuracy\": np.nan,\n",
    "                    \"test_accuracy\": np.nan,\n",
    "                    \"accuracy_gap\": np.nan,\n",
    "                    \"alpha\": np.nan,\n",
    "                    \"ERG_gap\": np.nan,\n",
    "                    \"num_traps\": np.nan,\n",
    "                }\n",
    "                overfit_records[pair] = fail_row\n",
    "                failed_pairs.add(pair)\n",
    "\n",
    "            # Persist after every pair update for robust restart behavior.\n",
    "            overfit_df = pd.DataFrame(list(overfit_records.values()))\n",
    "            combined_df = pd.concat([good_df, overfit_df], ignore_index=True, sort=False)\n",
    "            combined_df.to_csv(CHECKPOINT_AGGREGATED_CSV, index=False)\n",
    "\n",
    "    overfit_df = pd.DataFrame(list(overfit_records.values()))\n",
    "    for acc_col in [\"train_accuracy\", \"test_accuracy\", \"accuracy_gap\"]:\n",
    "        if acc_col not in overfit_df.columns:\n",
    "            overfit_df[acc_col] = np.nan\n",
    "        overfit_df[acc_col] = pd.to_numeric(overfit_df[acc_col], errors=\"coerce\")\n",
    "    overfit_df.to_csv(OVERFIT_RESULTS_CSV, index=False)\n",
    "\n",
    "    combined_df = pd.concat([good_df, overfit_df], ignore_index=True, sort=False)\n",
    "    combined_df.to_csv(CHECKPOINT_AGGREGATED_CSV, index=False)\n",
    "\n",
    "    print(\"Saved overfit-only checkpoint:\")\n",
    "    print(\" -\", OVERFIT_RESULTS_CSV)\n",
    "    print(\"Saved aggregated checkpoint:\")\n",
    "    print(\" -\", CHECKPOINT_AGGREGATED_CSV)\n",
    "    print(\n",
    "        \"Good rows:\",\n",
    "        len(good_df),\n",
    "        \"| Overfit rows:\",\n",
    "        len(overfit_df),\n",
    "        \"| Expected overfit rows:\",\n",
    "        expected_total,\n",
    "    )\n",
    "    if \"status\" in overfit_df.columns:\n",
    "        print(\"Overfit status counts:\")\n",
    "        display(overfit_df[\"status\"].value_counts(dropna=False))\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        display(\n",
    "            combined_df[\n",
    "                [\n",
    "                    \"case_type\", \"overfit_mode\", \"source\", \"dataset_uid\", \"train_accuracy\", \"test_accuracy\",\n",
    "                    \"accuracy_gap\", \"alpha\", \"ERG_gap\", \"num_traps\"\n",
    "                ]\n",
    "            ].sort_values([\"case_type\", \"source\", \"dataset_uid\", \"overfit_mode\"])\n",
    "        )\n",
    "\n",
    "        # Scatter plot: train vs test accuracy\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plot_df = combined_df.copy()\n",
    "        for case, marker in [(\"good\", \"o\"), (\"overfit\", \"x\")]:\n",
    "            sub = plot_df[plot_df[\"case_type\"] == case]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            plt.scatter(\n",
    "                sub[\"test_accuracy\"],\n",
    "                sub[\"train_accuracy\"],\n",
    "                alpha=0.7,\n",
    "                label=case,\n",
    "                marker=marker,\n",
    "            )\n",
    "        min_acc = np.nanmin([plot_df[\"test_accuracy\"].min(), plot_df[\"train_accuracy\"].min()])\n",
    "        max_acc = np.nanmax([plot_df[\"test_accuracy\"].max(), plot_df[\"train_accuracy\"].max()])\n",
    "        plt.plot([min_acc, max_acc], [min_acc, max_acc], linestyle=\"--\")\n",
    "        plt.xlabel(\"Test accuracy\")\n",
    "        plt.ylabel(\"Train accuracy\")\n",
    "        plt.title(\"Train vs Test accuracy: good vs overfit cases\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "a_SNj9ibYHgA"
   },
   "id": "a_SNj9ibYHgA",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!head -200 $CHECKPOINT_AGGREGATED_CSV\n"
   ],
   "metadata": {
    "id": "6hc1QW9HAoQc"
   },
   "id": "6hc1QW9HAoQc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 9) Per-overfit-mode accuracy visualizations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'combined_df' not in globals() or combined_df.empty:\n",
    "    print('No combined results available. Run the training/aggregation cells first.')\n",
    "else:\n",
    "    overfit_plot_df = combined_df[combined_df['case_type'] == 'overfit'].copy()\n",
    "    if overfit_plot_df.empty:\n",
    "        print('No overfit rows available for accuracy plots.')\n",
    "    else:\n",
    "        overfit_plot_df = overfit_plot_df[overfit_plot_df.get('status', 'completed').astype(str) == 'completed'].copy()\n",
    "        for acc_col in ['train_accuracy', 'test_accuracy']:\n",
    "            overfit_plot_df[acc_col] = pd.to_numeric(overfit_plot_df[acc_col], errors='coerce')\n",
    "        overfit_plot_df = overfit_plot_df.dropna(subset=['train_accuracy', 'test_accuracy'])\n",
    "        overfit_plot_df['generalization_gap'] = overfit_plot_df['train_accuracy'] - overfit_plot_df['test_accuracy']\n",
    "\n",
    "        if overfit_plot_df.empty:\n",
    "            print('No completed overfit rows with train/test accuracies available.')\n",
    "        else:\n",
    "            modes = [m for m in OVERFIT_MODES[:MAX_OVERFIT_CASES] if m in set(overfit_plot_df['overfit_mode'].astype(str))]\n",
    "            print(\n",
    "                f'Building overfit-mode accuracy plots for {len(modes)} modes '\n",
    "                f'with threshold RANDOM_SAMPLE_SIZE={RANDOM_SAMPLE_SIZE}.'\n",
    "            )\n",
    "\n",
    "            for mode in modes:\n",
    "                mode_df = overfit_plot_df.loc[overfit_plot_df['overfit_mode'] == mode].copy()\n",
    "                n_models = len(mode_df)\n",
    "                print(f'overfit_mode={mode} | completed models={n_models}')\n",
    "\n",
    "                if n_models <= RANDOM_SAMPLE_SIZE:\n",
    "                    print(\n",
    "                        f'Generating per-model train/test bar charts for mode={mode} '\n",
    "                        f'(n={n_models} <= {RANDOM_SAMPLE_SIZE}).'\n",
    "                    )\n",
    "                    mode_df = mode_df.sort_values(['source', 'dataset_uid']).reset_index(drop=True)\n",
    "                    for idx, row in mode_df.iterrows():\n",
    "                        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "                        bars = ax.bar(\n",
    "                            ['Train accuracy', 'Test accuracy'],\n",
    "                            [row['train_accuracy'], row['test_accuracy']],\n",
    "                            color=['tab:blue', 'tab:orange'],\n",
    "                        )\n",
    "                        ax.set_ylim(0, 1)\n",
    "                        ax.set_ylabel('Accuracy')\n",
    "                        ax.set_title(\n",
    "                            f\"{mode} | dataset={row['dataset_uid']} | \"\n",
    "                            f\"model {idx + 1}/{n_models}\"\n",
    "                        )\n",
    "                        ax.grid(axis='y', alpha=0.2)\n",
    "                        for bar in bars:\n",
    "                            height = bar.get_height()\n",
    "                            ax.text(\n",
    "                                bar.get_x() + bar.get_width() / 2,\n",
    "                                min(height + 0.02, 1.0),\n",
    "                                f'{height:.3f}',\n",
    "                                ha='center',\n",
    "                                va='bottom',\n",
    "                                fontsize=9,\n",
    "                            )\n",
    "                        fig.tight_layout()\n",
    "                        plt.show()\n",
    "                else:\n",
    "                    print(\n",
    "                        f'Generating generalization-gap histogram for mode={mode} '\n",
    "                        f'(n={n_models} > {RANDOM_SAMPLE_SIZE}).'\n",
    "                    )\n",
    "                    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "                    ax.hist(mode_df['generalization_gap'].values, bins=25, alpha=0.85, edgecolor='black')\n",
    "                    ax.set_title(\n",
    "                        f'Generalization gap histogram | mode={mode} | '\n",
    "                        f'n={n_models}'\n",
    "                    )\n",
    "                    ax.set_xlabel('Generalization gap (train_accuracy - test_accuracy)')\n",
    "                    ax.set_ylabel('Count')\n",
    "                    ax.grid(alpha=0.2)\n",
    "                    fig.tight_layout()\n",
    "                    plt.show()\n"
   ],
   "metadata": {
    "id": "_9cTHchdNwhS"
   },
   "id": "_9cTHchdNwhS",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "XGBWW_Catalog_Random100_XGBoost_Accuracy_WithOverfitCatalog.ipynb",
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "H100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}