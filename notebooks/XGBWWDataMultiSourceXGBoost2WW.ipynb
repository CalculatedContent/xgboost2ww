{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CalculatedContent/xgboost2ww/blob/main/notebooks/XGBWWDataMultiSourceXGBoost2WW.ipynb)\n\n# Multi-source xgbwwdata + xgboost2ww Experiments\n\nThis Colab notebook trains strong XGBoost classifiers across multiple dataset sources from `xgbwwdata`, converts each trained model to an `xgboost2ww` layer, and evaluates the resulting spectral diagnostics with WeightWatcher.\n\nKey goals:\n- Pull **N=20 datasets from each source** (`openml`, `pmlb`, `keel`, `libsvm`, `amlb`).\n- Train a high-accuracy XGBoost model using train-only CV (as in `GoodModelsXGBoost2WW.ipynb`).\n- Convert each model to a chosen xgboost2ww matrix (`W1`, `W2`, `W7`, or `W8`).\n- Run WeightWatcher with `ERG=True` and `randomize=True`.\n- Plot train/test accuracies versus `alpha`, `ERG_gap`, and `num_traps` in dedicated cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Experiment configuration\n",
    "MATRIX = \"W8\"  # @param [\"W1\", \"W2\", \"W7\", \"W8\"]\n",
    "\n",
    "DATA_SOURCES = [\"openml\", \"pmlb\", \"keel\", \"libsvm\", \"amlb\"]\n",
    "DATASETS_PER_SOURCE = 20\n",
    "TARGET_DATASETS = DATASETS_PER_SOURCE * len(DATA_SOURCES)\n",
    "\n",
    "CATALOG_CSV = \"/content/drive/MyDrive/xgbwwdata/catalog_checkpoint/dataset_catalog.csv\"\n",
    "RNG = 0\n",
    "\n",
    "TEST_SIZE = 0.20\n",
    "NFOLDS = 5\n",
    "T_TRAJ = 160\n",
    "\n",
    "MAX_ROWS = 60000\n",
    "MAX_FEATURES_GUARD = 50_000\n",
    "MAX_DENSE_ELEMENTS = int(2e8)\n",
    "\n",
    "GOOD_TRIALS = 5\n",
    "CV_MAX_ROUNDS = 3000\n",
    "CV_EARLY_STOP = 150\n",
    "MIN_GOOD_TEST_ACC = 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount Google Drive and create output directory\n",
    "from google.colab import drive\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "GDRIVE_DIR = \"/content/drive/MyDrive/xgboost2ww_runs\"\n",
    "os.makedirs(GDRIVE_DIR, exist_ok=True)\n",
    "print(\"Saving results under:\", GDRIVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies and xgbwwdata\n",
    "!apt-get -qq update && apt-get -qq install -y git\n",
    "\n",
    "%pip install -q -U pip setuptools wheel\n",
    "%pip install -q \"pandas==2.2.2\" xgboost weightwatcher scikit-learn scipy pyarrow xgboost2ww\n",
    "\n",
    "!rm -rf /content/repo_xgbwwdata\n",
    "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\n",
    "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\n",
    "\n",
    "import xgboost2ww\n",
    "import xgbwwdata\n",
    "print(\"xgboost2ww:\", getattr(xgboost2ww, \"__file__\", None))\n",
    "print(\"xgbwwdata:\", getattr(xgbwwdata, \"__file__\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports and shared helpers\n",
    "import warnings, time, gc\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse as sp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "import torch\n",
    "import weightwatcher as ww\n",
    "\n",
    "from xgbwwdata import load_dataset\n",
    "from xgboost2ww import convert\n",
    "\n",
    "rng = np.random.default_rng(RNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optional: GPU detection for XGBoost\n",
    "\n",
    "def xgb_gpu_available() -> bool:\n",
    "    try:\n",
    "        Xtmp = np.random.randn(256, 8).astype(np.float32)\n",
    "        ytmp = (Xtmp[:, 0] > 0).astype(np.int32)\n",
    "        dtmp = xgb.DMatrix(Xtmp, label=ytmp)\n",
    "        params = dict(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"gpu_hist\",\n",
    "            predictor=\"gpu_predictor\",\n",
    "            max_depth=2,\n",
    "            learning_rate=0.2,\n",
    "            seed=RNG,\n",
    "        )\n",
    "        _ = xgb.train(params=params, dtrain=dtmp, num_boost_round=5, verbose_eval=False)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "USE_GPU = xgb_gpu_available()\n",
    "print(\"XGBoost GPU available:\", USE_GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load dataset candidates from xgbwwdata catalog checkpoint CSV\n",
    "if not Path(CATALOG_CSV).exists():\n",
    "    raise FileNotFoundError(f\"Catalog CSV not found: {CATALOG_CSV}\")\n",
    "\n",
    "df_catalog = pd.read_csv(CATALOG_CSV)\n",
    "print(\"Catalog shape:\", df_catalog.shape)\n",
    "\n",
    "required_cols = {\"dataset_uid\", \"source\"}\n",
    "missing = required_cols - set(df_catalog.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Catalog is missing required columns: {missing}\")\n",
    "\n",
    "# Normalize source keys to avoid case/whitespace mismatches between config and catalog\n",
    "normalized_sources = [str(s).strip().lower() for s in DATA_SOURCES]\n",
    "df_catalog[\"source_norm\"] = df_catalog[\"source\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Keep only the configured sources (if present in the catalog)\n",
    "df_registry = df_catalog[df_catalog[\"source_norm\"].isin(normalized_sources)].copy()\n",
    "if len(df_registry) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No datasets found in catalog for selected DATA_SOURCES after normalization. \"\n",
    "        f\"Configured={normalized_sources}; available={sorted(df_catalog['source_norm'].dropna().unique())[:20]}\"\n",
    "    )\n",
    "\n",
    "# Keep binary-classification candidates when task metadata is available\n",
    "if \"task_type\" in df_registry.columns:\n",
    "    df_registry = df_registry[\n",
    "        df_registry[\"task_type\"].astype(str).str.contains(\"classification\", case=False, na=False)\n",
    "    ].copy()\n",
    "\n",
    "print(\"Candidates loaded from catalog:\", len(df_registry))\n",
    "preview_cols = [c for c in [\"source\", \"dataset_uid\", \"name\", \"task_type\"] if c in df_registry.columns]\n",
    "display(df_registry[preview_cols].head(10))\n",
    "print(df_registry[\"source_norm\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Training and WeightWatcher utilities\n",
    "import hashlib\n",
    "\n",
    "def stable_int_seed(value, mod=(2**31 - 1)):\n",
    "    digest = hashlib.sha256(str(value).encode(\"utf-8\")).digest()\n",
    "    return int.from_bytes(digest[:8], \"little\") % int(mod)\n",
    "\n",
    "\n",
    "def encode_binary_labels(y):\n",
    "    y = np.asarray(y)\n",
    "    classes, encoded = np.unique(y, return_inverse=True)\n",
    "    if len(classes) != 2:\n",
    "        raise ValueError(f\"Expected binary labels, got classes={classes}\")\n",
    "    return encoded.astype(np.int32), classes\n",
    "\n",
    "\n",
    "def pick_good_params_via_cv(Xtr, ytr, nfold=5, *, dataset_seed: int):\n",
    "    dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "    local_rng = np.random.default_rng(RNG + int(dataset_seed))\n",
    "\n",
    "    best = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    for _ in range(GOOD_TRIALS):\n",
    "        params = dict(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            seed=RNG,\n",
    "            learning_rate=float(10 ** local_rng.uniform(-2.0, -0.6)),\n",
    "            max_depth=int(local_rng.integers(2, 7)),\n",
    "            min_child_weight=float(10 ** local_rng.uniform(0.0, 2.0)),\n",
    "            subsample=float(local_rng.uniform(0.6, 0.9)),\n",
    "            colsample_bytree=float(local_rng.uniform(0.6, 0.9)),\n",
    "            reg_lambda=float(10 ** local_rng.uniform(0.0, 2.0)),\n",
    "            gamma=float(local_rng.uniform(0.0, 0.5)),\n",
    "        )\n",
    "        if USE_GPU:\n",
    "            params[\"tree_method\"] = \"gpu_hist\"\n",
    "            params[\"predictor\"] = \"gpu_predictor\"\n",
    "\n",
    "        cv = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=CV_MAX_ROUNDS,\n",
    "            nfold=nfold,\n",
    "            stratified=True,\n",
    "            early_stopping_rounds=CV_EARLY_STOP,\n",
    "            seed=RNG,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        score = float(cv[\"test-logloss-mean\"].iloc[-1])\n",
    "        rounds = int(len(cv))\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best = (params, rounds, score)\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def train_eval_fulltrain(Xtr, ytr, Xte, yte, params, rounds):\n",
    "    dtr = xgb.DMatrix(Xtr, label=ytr)\n",
    "    dte = xgb.DMatrix(Xte, label=yte)\n",
    "\n",
    "    bst = xgb.train(params=params, dtrain=dtr, num_boost_round=rounds, verbose_eval=False)\n",
    "\n",
    "    m_tr = bst.predict(dtr, output_margin=True).astype(np.float32)\n",
    "    p_tr = 1.0 / (1.0 + np.exp(-m_tr))\n",
    "    train_acc = float(accuracy_score(ytr, (p_tr >= 0.5).astype(int)))\n",
    "\n",
    "    m_te = bst.predict(dte, output_margin=True).astype(np.float32)\n",
    "    p_te = 1.0 / (1.0 + np.exp(-m_te))\n",
    "    test_acc = float(accuracy_score(yte, (p_te >= 0.5).astype(int)))\n",
    "    test_loss = float(log_loss(yte, np.vstack([1 - p_te, p_te]).T, labels=[0, 1]))\n",
    "\n",
    "    return train_acc, test_acc, test_loss, bst\n",
    "\n",
    "\n",
    "def ww_metrics_from_layer(layer):\n",
    "    watcher = ww.WeightWatcher(model=layer)\n",
    "    details_df = watcher.analyze(randomize=True, ERG=True, plot=False)\n",
    "    alpha = float(details_df[\"alpha\"].iloc[0]) if \"alpha\" in details_df.columns else np.nan\n",
    "    traps = float(details_df[\"num_traps\"].iloc[0]) if \"num_traps\" in details_df.columns else np.nan\n",
    "    ERG_gap = float(details_df[\"ERG_gap\"].iloc[0]) if \"ERG_gap\" in details_df.columns else np.nan\n",
    "    return alpha, traps, ERG_gap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run experiment: keep up to N datasets per source\n",
    "import collections\n",
    "\n",
    "rows = []\n",
    "rows_below_threshold = []\n",
    "accepted_by_source = {str(s).strip().lower(): 0 for s in DATA_SOURCES}\n",
    "skip = collections.Counter()\n",
    "\n",
    "# Adaptive fallback so downstream plotting still works when thresholds are too strict.\n",
    "fallback_min_keep = max(10, len(DATA_SOURCES))\n",
    "\n",
    "print(\"Registry size:\", len(df_registry))\n",
    "print(\"Registry sources:\\n\", df_registry[\"source_norm\"].value_counts(dropna=False))\n",
    "print(\"DATA_SOURCES:\", DATA_SOURCES)\n",
    "print(\"DATASETS_PER_SOURCE:\", DATASETS_PER_SOURCE)\n",
    "print(\"TARGET_DATASETS:\", TARGET_DATASETS)\n",
    "\n",
    "registry_source_keys = set(df_registry[\"source_norm\"].dropna().astype(str).unique()) if \"source_norm\" in df_registry.columns else set()\n",
    "config_source_keys = set(accepted_by_source.keys())\n",
    "if len(df_registry) == 0:\n",
    "    print(\"df_registry is empty after filtering\")\n",
    "elif registry_source_keys and registry_source_keys.isdisjoint(config_source_keys):\n",
    "    print(\"source keys mismatch\")\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "loop_iterations = 0\n",
    "for i, (_, rec) in enumerate(df_registry.iterrows(), start=1):\n",
    "    loop_iterations += 1\n",
    "    source = str(rec.get(\"source_norm\", rec.get(\"source\", \"unknown\"))).strip().lower()\n",
    "    dataset_uid = rec.get(\"dataset_uid\", \"<missing_uid>\")\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(f\"progress i={i} dataset_uid={dataset_uid} source={source} accepted={accepted_by_source.get(source, 'n/a')}\", flush=True)\n",
    "\n",
    "    if source not in accepted_by_source:\n",
    "        skip[\"source_not_in_config\"] += 1\n",
    "        continue\n",
    "    if accepted_by_source[source] >= DATASETS_PER_SOURCE:\n",
    "        skip[\"source_already_full\"] += 1\n",
    "        continue\n",
    "\n",
    "    if all(v >= DATASETS_PER_SOURCE for v in accepted_by_source.values()):\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        X, y, meta = load_dataset(dataset_uid=dataset_uid, source=source, preprocess=True)\n",
    "    except Exception as e:\n",
    "        skip[\"load_fail\"] += 1\n",
    "        print(\"SKIP load:\", dataset_uid, type(e).__name__, e)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        y, class_values = encode_binary_labels(y)\n",
    "    except Exception:\n",
    "        skip[\"nonbinary_labels\"] += 1\n",
    "        continue\n",
    "\n",
    "    if int(X.shape[1]) > MAX_FEATURES_GUARD:\n",
    "        skip[\"too_many_features_guard\"] += 1\n",
    "        continue\n",
    "\n",
    "    if np.min(np.bincount(y)) < 2:\n",
    "        skip[\"min_class_count_lt2\"] += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        tr_idx, te_idx = train_test_split(\n",
    "            np.arange(len(y)),\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=RNG,\n",
    "            stratify=y,\n",
    "        )\n",
    "    except Exception:\n",
    "        skip[\"train_fail\"] += 1\n",
    "        continue\n",
    "\n",
    "    Xtr, Xte = X[tr_idx], X[te_idx]\n",
    "    ytr, yte = y[tr_idx], y[te_idx]\n",
    "\n",
    "    is_sparse = sp.issparse(Xtr)\n",
    "    if is_sparse:\n",
    "        Xtr = Xtr.tocsr().astype(np.float32)\n",
    "        Xte = Xte.tocsr().astype(np.float32)\n",
    "        MAX_NNZ = MAX_DENSE_ELEMENTS\n",
    "        if int(Xtr.nnz) > MAX_NNZ:\n",
    "            skip[\"sparse_guard_triggered\"] += 1\n",
    "            continue\n",
    "    else:\n",
    "        Xtr = np.asarray(Xtr, dtype=np.float32)\n",
    "        Xte = np.asarray(Xte, dtype=np.float32)\n",
    "        if int(Xtr.size) > MAX_DENSE_ELEMENTS:\n",
    "            skip[\"dense_guard_triggered\"] += 1\n",
    "            continue\n",
    "\n",
    "    seed_from_uid = stable_int_seed(dataset_uid)\n",
    "    try:\n",
    "        good_params, good_rounds, good_cv_logloss = pick_good_params_via_cv(\n",
    "            Xtr, ytr, nfold=NFOLDS, dataset_seed=seed_from_uid\n",
    "        )\n",
    "\n",
    "        good_train_acc, good_test_acc, good_test_loss, bst = train_eval_fulltrain(\n",
    "            Xtr, ytr, Xte, yte, good_params, good_rounds\n",
    "        )\n",
    "    except Exception as e:\n",
    "        skip[\"train_fail\"] += 1\n",
    "        print(\"SKIP train:\", dataset_uid, type(e).__name__, e)\n",
    "        del X, y, Xtr, Xte, ytr, yte\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    base_row = dict(\n",
    "        dataset_uid=dataset_uid,\n",
    "        source=source,\n",
    "        dataset=meta.get(\"name\", rec.get(\"name\", dataset_uid)),\n",
    "        original_classes=str(tuple(class_values.tolist())),\n",
    "        n_rows_total=int(X.shape[0]),\n",
    "        n_train=int(Xtr.shape[0]),\n",
    "        n_test=int(Xte.shape[0]),\n",
    "        n_features=int(X.shape[1]),\n",
    "        rounds=int(good_rounds),\n",
    "        cv_logloss=float(good_cv_logloss),\n",
    "        good_train_acc=float(good_train_acc),\n",
    "        good_test_acc=float(good_test_acc),\n",
    "        good_test_loss=float(good_test_loss),\n",
    "    )\n",
    "\n",
    "    if good_test_acc < MIN_GOOD_TEST_ACC:\n",
    "        skip[\"below_min_test_acc\"] += 1\n",
    "        rows_below_threshold.append(base_row)\n",
    "        del bst, X, y, Xtr, Xte, ytr, yte\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        layer_W = convert(\n",
    "            model=bst,\n",
    "            data=Xtr,\n",
    "            labels=ytr,\n",
    "            W=MATRIX,\n",
    "            nfolds=NFOLDS,\n",
    "            t_points=T_TRAJ,\n",
    "            random_state=RNG,\n",
    "            train_params=good_params,\n",
    "            num_boost_round=good_rounds,\n",
    "            multiclass=\"error\",\n",
    "            return_type=\"torch\",\n",
    "            verbose=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        skip[\"convert_fail\"] += 1\n",
    "        print(\"SKIP convert:\", dataset_uid, type(e).__name__, e)\n",
    "        del bst, X, y, Xtr, Xte, ytr, yte\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    alpha_W, traps_W, ERG_gap_W = ww_metrics_from_layer(layer_W)\n",
    "    rows.append(dict(base_row, alpha_W=float(alpha_W), traps_W=float(traps_W), ERG_gap_W=float(ERG_gap_W)))\n",
    "\n",
    "    accepted_by_source[source] += 1\n",
    "    kept_total = sum(accepted_by_source.values())\n",
    "    elapsed = (time.time() - t0) / 60.0\n",
    "    print(\n",
    "        f\"[{kept_total}/{TARGET_DATASETS}] src={source} ({accepted_by_source[source]}/{DATASETS_PER_SOURCE}) \"\n",
    "        f\"{meta.get('name', dataset_uid)} | train/test={good_train_acc:.3f}/{good_test_acc:.3f} \"\n",
    "        f\"| \u03b1={alpha_W:.2f} traps={traps_W:.1f} ERG_gap={ERG_gap_W:.2f} | elapsed={elapsed:.1f} min\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    del bst, layer_W, X, y, Xtr, Xte, ytr, yte\n",
    "    gc.collect()\n",
    "\n",
    "if loop_iterations == 0:\n",
    "    if len(df_registry) == 0:\n",
    "        print(\"df_registry is empty after filtering\")\n",
    "    else:\n",
    "        print(\"source keys mismatch\")\n",
    "\n",
    "if len(rows) == 0 and len(rows_below_threshold) > 0:\n",
    "    print(\n",
    "        f\"No datasets passed MIN_GOOD_TEST_ACC={MIN_GOOD_TEST_ACC:.3f}. \"\n",
    "        f\"Falling back to top-{fallback_min_keep} by test accuracy for diagnostics. \"\n",
    "        f\"below-threshold candidates={len(rows_below_threshold)}\"\n",
    "    )\n",
    "    fallback_df = pd.DataFrame(rows_below_threshold).sort_values(\"good_test_acc\", ascending=False).head(fallback_min_keep)\n",
    "    fallback_df[\"alpha_W\"] = np.nan\n",
    "    fallback_df[\"traps_W\"] = np.nan\n",
    "    fallback_df[\"ERG_gap_W\"] = np.nan\n",
    "    fallback_df[\"fallback_only\"] = True\n",
    "    rows = fallback_df.to_dict(orient=\"records\")\n",
    "\n",
    "skip_summary = dict(sorted(skip.items(), key=lambda kv: kv[1], reverse=True))\n",
    "print(\"Skip counters:\", skip_summary)\n",
    "print(\"Accepted per source:\", accepted_by_source)\n",
    "\n",
    "df_good = pd.DataFrame(rows)\n",
    "print(f\"DONE. datasets_kept={df_good['dataset_uid'].nunique() if len(df_good) else 0} rows={len(df_good)}\")\n",
    "display(df_good.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot train/test accuracies vs alpha_W\n",
    "if len(df_good) == 0:\n",
    "    print(\"No datasets kept. Try lowering MIN_GOOD_TEST_ACC or reviewing the catalog selection.\")\n",
    "else:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(df_good[\"alpha_W\"], df_good[\"good_train_acc\"], label=\"train_acc\", alpha=0.8)\n",
    "    plt.scatter(df_good[\"alpha_W\"], df_good[\"good_test_acc\"], label=\"test_acc\", alpha=0.8)\n",
    "    plt.xlabel(\"alpha_W\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(f\"Train/Test Accuracy vs alpha ({MATRIX})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot train/test accuracies vs ERG_gap_W\n",
    "if len(df_good) == 0:\n",
    "    print(\"No datasets kept. Try lowering MIN_GOOD_TEST_ACC or reviewing the catalog selection.\")\n",
    "else:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(df_good[\"ERG_gap_W\"], df_good[\"good_train_acc\"], label=\"train_acc\", alpha=0.8)\n",
    "    plt.scatter(df_good[\"ERG_gap_W\"], df_good[\"good_test_acc\"], label=\"test_acc\", alpha=0.8)\n",
    "    plt.xlabel(\"ERG_gap_W\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(f\"Train/Test Accuracy vs ERG_gap ({MATRIX})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot train/test accuracies vs traps_W (num_traps)\n",
    "if len(df_good) == 0:\n",
    "    print(\"No datasets kept. Try lowering MIN_GOOD_TEST_ACC or reviewing the catalog selection.\")\n",
    "else:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(df_good[\"traps_W\"], df_good[\"good_train_acc\"], label=\"train_acc\", alpha=0.8)\n",
    "    plt.scatter(df_good[\"traps_W\"], df_good[\"good_test_acc\"], label=\"test_acc\", alpha=0.8)\n",
    "    plt.xlabel(\"traps_W\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(f\"Train/Test Accuracy vs num_traps ({MATRIX})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Additional structural diagnostics\n",
    "if len(df_good) == 0:\n",
    "    print(\"No results to plot.\")\n",
    "else:\n",
    "    plt.figure(); plt.hist(df_good[\"alpha_W\"].dropna().values, bins=30)\n",
    "    plt.xlabel(\"alpha_W\"); plt.ylabel(\"count\"); plt.title(f\"Histogram of alpha({MATRIX})\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    plt.figure(); plt.hist(df_good[\"traps_W\"].dropna().values, bins=30)\n",
    "    plt.xlabel(\"traps_W\"); plt.ylabel(\"count\"); plt.title(f\"Histogram of traps({MATRIX})\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    plt.figure(); plt.hist(df_good[\"ERG_gap_W\"].dropna().values, bins=30)\n",
    "    plt.xlabel(\"ERG_gap_W\"); plt.ylabel(\"count\"); plt.title(f\"Histogram of ERG_gap({MATRIX})\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Save results to Google Drive\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RESULTS_FEATHER = os.path.join(GDRIVE_DIR, f\"{MATRIX}_multisource_results_{ts}.feather\")\n",
    "\n",
    "df_good.to_feather(RESULTS_FEATHER)\n",
    "print(f\"Saved {len(df_good)} rows to: {RESULTS_FEATHER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reload latest results from Google Drive and re-plot summary\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(GDRIVE_DIR, f\"{MATRIX}_multisource_results_*.feather\")))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No {MATRIX}_multisource_results_*.feather files found in {GDRIVE_DIR}\")\n",
    "\n",
    "RESULTS_FEATHER = files[-1]\n",
    "print(\"Loading:\", RESULTS_FEATHER)\n",
    "\n",
    "df = pd.read_feather(RESULTS_FEATHER)\n",
    "print(\"Rows:\", len(df), \"| Cols:\", len(df.columns))\n",
    "display(df.head(10))\n",
    "\n",
    "if \"good_test_acc\" in df.columns:\n",
    "    df = df.sort_values(\"good_test_acc\", ascending=False)\n",
    "\n",
    "plt.figure(); plt.scatter(df[\"good_test_acc\"].values, df[\"alpha_W\"].values)\n",
    "plt.xlabel(\"good_test_acc\"); plt.ylabel(\"alpha_W\")\n",
    "plt.title(f\"alpha({MATRIX}) vs test accuracy\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "summary_cols = [c for c in [\n",
    "    \"dataset\", \"dataset_uid\", \"source\", \"good_train_acc\", \"good_test_acc\",\n",
    "    \"alpha_W\", \"traps_W\", \"ERG_gap_W\", \"rounds\"\n",
    "] if c in df.columns]\n",
    "print(\"Top 15 by test accuracy:\")\n",
    "display(df[summary_cols].head(15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}