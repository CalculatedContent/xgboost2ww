{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CalculatedContent/xgboost2ww/blob/main/notebooks/XGBWWDataMultiSourceXGBoost2WWHyperparamSweep.ipynb)\n\n",
    "# Multi-source xgbwwdata + xgboost2ww Hyperparameter Sweep Experiments\n\n",
    "This Colab notebook is a hyperparameter-sweep variant of `XGBWWDataMultiSourceXGBoost2WW.ipynb`.\n\n",
    "Key goals:\n",
    "- Randomly select **10 datasets** from multi-source `xgbwwdata` candidates.\n",
    "- For each dataset, evaluate **10+ distinct hyperparameter settings**.\n",
    "- For each hyperparameter setting, train **5 independent repeats** for error bars.\n",
    "- Use a strict hold-out test split to measure train/test accuracy (no tuning on test set).\n",
    "- Convert each trained model with `xgboost2ww` and run WeightWatcher `analyze(randomize=True, ERG_gap=True)`.\n",
    "- Measure `alpha`, `ERG_gap`, and `num_traps`, and plot each metric vs train/test accuracy with error bars:\n",
    "  - **Per model:** 3 plots \u00d7 10 models = 30 plots.\n",
    "  - **Combined:** 3 aggregate plots across all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfg"
   },
   "outputs": [],
   "source": [
    "#@title Experiment configuration\n",
    "MATRIX = \"W8\"  # @param [\"W1\", \"W2\", \"W7\", \"W8\"]\n",
    "\n",
    "# Path to xgbwwdata checkpoint/data-file list CSV (set this to your local path in Colab)\n",
    "CATALOG_CSV = '/content/repo_xgbwwdata/checkpoint/data-file-list.csv'\n",
    "SAMPLES_PER_SOURCE = 200\n",
    "\n",
    "N_MODELS = 10\n",
    "N_HPARAM_SETTINGS = 10\n",
    "N_REPEATS = 5\n",
    "\n",
    "RNG = 7\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "MAX_ROWS = 60000\n",
    "MAX_FEATURES_GUARD = 50_000\n",
    "MAX_DENSE_ELEMENTS = int(2e8)\n",
    "\n",
    "MAX_BOOST_ROUNDS = 1200\n",
    "EARLY_STOPPING_ROUNDS = 80\n",
    "\n",
    "MIN_CLASSES = 2\n",
    "MAX_CLASSES = 2  # binary classification only for accuracy comparability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount Google Drive and create output directory\n",
    "from google.colab import drive\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "GDRIVE_DIR = '/content/drive/MyDrive/xgboost2ww_runs'\n",
    "os.makedirs(GDRIVE_DIR, exist_ok=True)\n",
    "print('Saving results under:', GDRIVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies and xgbwwdata\n",
    "!apt-get -qq update && apt-get -qq install -y git\n",
    "\n",
    "%pip install -q -U pip setuptools wheel\n",
    "%pip install -q weightwatcher xgboost2ww xgboost scikit-learn scipy pandas pyarrow matplotlib\n",
    "\n",
    "!rm -rf /content/repo_xgbwwdata\n",
    "!git clone https://github.com/CalculatedContent/xgbwwdata.git /content/repo_xgbwwdata\n",
    "%run /content/repo_xgbwwdata/scripts/colab_install.py --repo /content/repo_xgbwwdata\n",
    "\n",
    "import xgbwwdata\n",
    "import xgboost2ww\n",
    "import weightwatcher\n",
    "print('xgbwwdata:', getattr(xgbwwdata, '__file__', None))\n",
    "print('xgboost2ww:', getattr(xgboost2ww, '__file__', None))\n",
    "print('weightwatcher:', getattr(weightwatcher, '__file__', None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports and shared helpers\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import weightwatcher as ww\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgbwwdata import load_dataset\n",
    "from xgboost2ww import convert\n",
    "\n",
    "rng = np.random.default_rng(RNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Optional: GPU detection for XGBoost\n",
    "def xgb_gpu_available() -> bool:\n",
    "    try:\n",
    "        Xtmp = np.random.randn(256, 8).astype(np.float32)\n",
    "        ytmp = (Xtmp[:, 0] > 0).astype(np.int32)\n",
    "        dtmp = xgb.DMatrix(Xtmp, label=ytmp)\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'predictor': 'gpu_predictor',\n",
    "            'max_depth': 2,\n",
    "            'learning_rate': 0.2,\n",
    "            'seed': RNG,\n",
    "        }\n",
    "        _ = xgb.train(params=params, dtrain=dtmp, num_boost_round=5, verbose_eval=False)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "USE_GPU = xgb_gpu_available()\n",
    "print('XGBoost GPU available:', USE_GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load checkpoint/data-file list and sample classification datasets\n",
    "if not Path(CATALOG_CSV).exists():\n",
    "    raise FileNotFoundError(f'Catalog CSV not found: {CATALOG_CSV}')\n",
    "\n",
    "df_catalog = pd.read_csv(CATALOG_CSV)\n",
    "print('Catalog shape:', df_catalog.shape)\n",
    "\n",
    "required_cols = {\"dataset_uid\", \"source\", \"task_type\"}\n",
    "missing = required_cols - set(df_catalog.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Catalog is missing required columns: {missing}\")\n",
    "\n",
    "# Accuracy is for classification; keep classification-like tasks\n",
    "df_cls = df_catalog[\n",
    "    df_catalog[\"task_type\"].astype(str).str.contains(\"classification\", case=False, na=False)\n",
    "].copy()\n",
    "if df_cls.empty:\n",
    "    raise ValueError('No classification datasets found in catalog.')\n",
    "\n",
    "# Sample up to SAMPLES_PER_SOURCE per source\n",
    "def sample_per_source(group):\n",
    "    n = min(SAMPLES_PER_SOURCE, len(group))\n",
    "    return group.sample(n=n, random_state=RNG)\n",
    "\n",
    "df_pick = (\n",
    "    df_cls.groupby('source', group_keys=False)\n",
    "    .apply(sample_per_source)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print('Selected datasets:', len(df_pick))\n",
    "preview_cols = [c for c in ['source', 'dataset_uid', 'name', 'task_type'] if c in df_pick.columns]\n",
    "display(df_pick[preview_cols].sort_values(['source', 'dataset_uid']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select 10 random binary datasets and build hyperparameter grid\n",
    "def is_binary_record(rec: pd.Series) -> bool:\n",
    "    nc = rec.get('n_classes', None)\n",
    "    if pd.isna(nc):\n",
    "        return True\n",
    "    return int(nc) >= MIN_CLASSES and int(nc) <= MAX_CLASSES\n",
    "\n",
    "df_pool = df_pick[df_pick.apply(is_binary_record, axis=1)].copy()\n",
    "if len(df_pool) < N_MODELS:\n",
    "    raise RuntimeError(f'Not enough binary datasets ({len(df_pool)}) for N_MODELS={N_MODELS}.')\n",
    "\n",
    "df_models = df_pool.sample(n=N_MODELS, random_state=RNG).reset_index(drop=True)\n",
    "display(df_models[['source', 'dataset_uid', 'name']].head(N_MODELS))\n",
    "\n",
    "def sample_hparams(base_seed: int, n_settings: int):\n",
    "    local_rng = np.random.default_rng(base_seed)\n",
    "    settings = []\n",
    "    for hp_id in range(n_settings):\n",
    "        s = {\n",
    "            'hp_id': hp_id,\n",
    "            'learning_rate': float(local_rng.uniform(0.03, 0.20)),\n",
    "            'max_depth': int(local_rng.integers(3, 8)),\n",
    "            'min_child_weight': float(local_rng.uniform(1.0, 8.0)),\n",
    "            'subsample': float(local_rng.uniform(0.70, 0.95)),\n",
    "            'colsample_bytree': float(local_rng.uniform(0.70, 0.95)),\n",
    "            'gamma': float(local_rng.uniform(0.0, 0.6)),\n",
    "            'reg_alpha': float(local_rng.uniform(0.0, 0.8)),\n",
    "            'reg_lambda': float(local_rng.uniform(0.8, 4.0)),\n",
    "            'max_delta_step': int(local_rng.integers(0, 3)),\n",
    "        }\n",
    "        settings.append(s)\n",
    "    return settings\n",
    "\n",
    "hparam_bank = {\n",
    "    rec['dataset_uid']: sample_hparams(base_seed=RNG + i * 97, n_settings=N_HPARAM_SETTINGS)\n",
    "    for i, rec in df_models.iterrows()\n",
    "}\n",
    "print('Prepared hyperparameter settings per dataset:', N_HPARAM_SETTINGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train repeated models, convert with xgboost2ww, run WeightWatcher\n",
    "def ww_metrics_from_layer(layer):\n",
    "    details = ww.WeightWatcher(model=layer).analyze(\n",
    "        randomize=True,\n",
    "        ERG=True,\n",
    "        ERG_gap=True,\n",
    "        min_evals=10,\n",
    "        plot=False,\n",
    "    )\n",
    "    if len(details) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    row = details.iloc[0]\n",
    "    return float(row.get('alpha', np.nan)), float(row.get('num_traps', np.nan)), float(row.get('ERG_gap', np.nan))\n",
    "\n",
    "rows = []\n",
    "t0 = time.time()\n",
    "\n",
    "for midx, rec in df_models.iterrows():\n",
    "    dataset_uid = rec['dataset_uid']\n",
    "    source = rec['source']\n",
    "    name = rec.get('name', dataset_uid)\n",
    "\n",
    "    try:\n",
    "        X, y, meta = load_dataset(dataset_uid=dataset_uid, source=source, preprocess=True)\n",
    "    except Exception as e:\n",
    "        print('SKIP load:', dataset_uid, type(e).__name__, e)\n",
    "        continue\n",
    "\n",
    "    if len(np.unique(y)) != 2:\n",
    "        print('SKIP non-binary at load-time:', dataset_uid)\n",
    "        continue\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RNG + midx, stratify=y\n",
    "    )\n",
    "\n",
    "    dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "    dtest = xgb.DMatrix(Xte, label=yte)\n",
    "\n",
    "    for hp in hparam_bank[dataset_uid]:\n",
    "        hp_id = hp['hp_id']\n",
    "        for rep in range(N_REPEATS):\n",
    "            seed = RNG + midx * 10000 + hp_id * 100 + rep\n",
    "            params = dict(\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='logloss',\n",
    "                tree_method='hist',\n",
    "                seed=seed,\n",
    "                learning_rate=hp['learning_rate'],\n",
    "                max_depth=hp['max_depth'],\n",
    "                min_child_weight=hp['min_child_weight'],\n",
    "                subsample=hp['subsample'],\n",
    "                colsample_bytree=hp['colsample_bytree'],\n",
    "                gamma=hp['gamma'],\n",
    "                reg_alpha=hp['reg_alpha'],\n",
    "                reg_lambda=hp['reg_lambda'],\n",
    "                max_delta_step=hp['max_delta_step'],\n",
    "            )\n",
    "            if USE_GPU:\n",
    "                params.update(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "\n",
    "            evals_result = {}\n",
    "            bst = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=MAX_BOOST_ROUNDS,\n",
    "                evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "                early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=False,\n",
    "            )\n",
    "\n",
    "            best_ntree = bst.best_ntree_limit if hasattr(bst, 'best_ntree_limit') else bst.best_iteration + 1\n",
    "            p_tr = bst.predict(dtrain, iteration_range=(0, best_ntree))\n",
    "            p_te = bst.predict(dtest, iteration_range=(0, best_ntree))\n",
    "            tr_acc = accuracy_score(ytr, (p_tr >= 0.5).astype(int))\n",
    "            te_acc = accuracy_score(yte, (p_te >= 0.5).astype(int))\n",
    "\n",
    "            layer_W = convert(\n",
    "                model=bst,\n",
    "                data=Xtr,\n",
    "                labels=ytr,\n",
    "                W=MATRIX,\n",
    "                nfolds=5,\n",
    "                t_points=120,\n",
    "                random_state=seed,\n",
    "                train_params=params,\n",
    "                num_boost_round=int(best_ntree),\n",
    "                multiclass='error',\n",
    "                return_type='torch',\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            alpha_W, traps_W, ERG_gap_W = ww_metrics_from_layer(layer_W)\n",
    "\n",
    "            rows.append({\n",
    "                'dataset_uid': dataset_uid,\n",
    "                'dataset': meta.get('name', name),\n",
    "                'source': source,\n",
    "                'hp_id': hp_id,\n",
    "                'repeat_id': rep,\n",
    "                'best_rounds': int(best_ntree),\n",
    "                'train_acc': float(tr_acc),\n",
    "                'test_acc': float(te_acc),\n",
    "                'alpha_W': float(alpha_W),\n",
    "                'traps_W': float(traps_W),\n",
    "                'ERG_gap_W': float(ERG_gap_W),\n",
    "                **{k: v for k, v in hp.items() if k != 'hp_id'},\n",
    "            })\n",
    "\n",
    "            print(f\"model={midx+1}/{N_MODELS} {dataset_uid} hp={hp_id+1}/{N_HPARAM_SETTINGS} rep={rep+1}/{N_REPEATS} \\\n",
    "                  f\"train={tr_acc:.3f} test={te_acc:.3f} alpha={alpha_W:.2f} traps={traps_W:.1f} ERG={ERG_gap_W:.2f}\")\n",
    "\n",
    "            del bst, layer_W\n",
    "            gc.collect()\n",
    "\n",
    "    del X, y, Xtr, Xte, ytr, yte, dtrain, dtest\n",
    "    gc.collect()\n",
    "\n",
    "df_runs = pd.DataFrame(rows)\n",
    "elapsed_min = (time.time() - t0) / 60.0\n",
    "print(f'Finished runs: {len(df_runs)} rows in {elapsed_min:.1f} minutes')\n",
    "display(df_runs.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Aggregate repeats for error bars (per dataset, per hyperparameter setting)\n",
    "if len(df_runs) == 0:\n",
    "    raise RuntimeError('No runs completed.')\n",
    "\n",
    "agg = (\n",
    "    df_runs\n",
    "    .groupby(['dataset_uid', 'dataset', 'source', 'hp_id'], as_index=False)\n",
    "    .agg(\n",
    "        train_acc_mean=('train_acc', 'mean'),\n",
    "        train_acc_std=('train_acc', 'std'),\n",
    "        test_acc_mean=('test_acc', 'mean'),\n",
    "        test_acc_std=('test_acc', 'std'),\n",
    "        alpha_mean=('alpha_W', 'mean'),\n",
    "        alpha_std=('alpha_W', 'std'),\n",
    "        traps_mean=('traps_W', 'mean'),\n",
    "        traps_std=('traps_W', 'std'),\n",
    "        ERG_gap_mean=('ERG_gap_W', 'mean'),\n",
    "        ERG_gap_std=('ERG_gap_W', 'std'),\n",
    "        repeats=('repeat_id', 'nunique'),\n",
    "    )\n",
    "    .sort_values(['dataset_uid', 'hp_id'])\n",
    ")\n",
    "\n",
    "for c in [c for c in agg.columns if c.endswith('_std')]:\n",
    "    agg[c] = agg[c].fillna(0.0)\n",
    "\n",
    "display(agg.head(20))\n",
    "print('Aggregated rows:', len(agg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Per-model plots: metric vs train/test accuracy (3 plots per model, with error bars)\n",
    "metrics = [\n",
    "    ('alpha_mean', 'alpha_std', 'alpha'),\n",
    "    ('ERG_gap_mean', 'ERG_gap_std', 'ERG_gap'),\n",
    "    ('traps_mean', 'traps_std', 'num_traps'),\n",
    "]\n",
    "\n",
    "def plot_metric_vs_acc(df_sub, x_col, xerr_col, x_label, title_prefix=''):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.errorbar(\n",
    "        df_sub[x_col], df_sub['train_acc_mean'],\n",
    "        xerr=df_sub[xerr_col], yerr=df_sub['train_acc_std'],\n",
    "        fmt='o', capsize=3, alpha=0.8, label='train_acc'\n",
    "    )\n",
    "    plt.errorbar(\n",
    "        df_sub[x_col], df_sub['test_acc_mean'],\n",
    "        xerr=df_sub[xerr_col], yerr=df_sub['test_acc_std'],\n",
    "        fmt='s', capsize=3, alpha=0.8, label='test_acc'\n",
    "    )\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title(f'{title_prefix}: accuracy vs {x_label}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for dataset_uid, df_sub in agg.groupby('dataset_uid', sort=False):\n",
    "    ds_name = df_sub['dataset'].iloc[0]\n",
    "    title_prefix = f'{ds_name} ({dataset_uid})'\n",
    "    for x_col, xerr_col, x_label in metrics:\n",
    "        plot_metric_vs_acc(df_sub, x_col, xerr_col, x_label, title_prefix=title_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Combined plots across all models (one for each WW metric, with error bars)\n",
    "for x_col, xerr_col, x_label in metrics:\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.errorbar(\n",
    "        agg[x_col], agg['train_acc_mean'],\n",
    "        xerr=agg[xerr_col], yerr=agg['train_acc_std'],\n",
    "        fmt='o', capsize=2, alpha=0.6, label='train_acc'\n",
    "    )\n",
    "    plt.errorbar(\n",
    "        agg[x_col], agg['test_acc_mean'],\n",
    "        xerr=agg[xerr_col], yerr=agg['test_acc_std'],\n",
    "        fmt='s', capsize=2, alpha=0.6, label='test_acc'\n",
    "    )\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title(f'Combined across all models: accuracy vs {x_label}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Save run-level and aggregated results to Google Drive\n",
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUNS_FEATHER = os.path.join(GDRIVE_DIR, f'{MATRIX}_multisource_hp_sweep_runs_{ts}.feather')\n",
    "AGG_FEATHER = os.path.join(GDRIVE_DIR, f'{MATRIX}_multisource_hp_sweep_agg_{ts}.feather')\n",
    "\n",
    "df_runs.to_feather(RUNS_FEATHER)\n",
    "agg.to_feather(AGG_FEATHER)\n",
    "print('Saved run-level:', RUNS_FEATHER, '| rows=', len(df_runs))\n",
    "print('Saved aggregated:', AGG_FEATHER, '| rows=', len(agg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Reload latest aggregated results and quick summary table\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(GDRIVE_DIR, f'{MATRIX}_multisource_hp_sweep_agg_*.feather')))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f'No aggregated files found in {GDRIVE_DIR}')\n",
    "\n",
    "latest = files[-1]\n",
    "print('Loading:', latest)\n",
    "df_latest = pd.read_feather(latest)\n",
    "display(df_latest.head(20))\n",
    "\n",
    "summary = (\n",
    "    df_latest\n",
    "    .groupby(['dataset_uid', 'dataset', 'source'], as_index=False)\n",
    "    .agg(\n",
    "        best_test_acc=('test_acc_mean', 'max'),\n",
    "        best_train_acc=('train_acc_mean', 'max'),\n",
    "        mean_alpha=('alpha_mean', 'mean'),\n",
    "        mean_erg_gap=('ERG_gap_mean', 'mean'),\n",
    "        mean_traps=('traps_mean', 'mean'),\n",
    "    )\n",
    "    .sort_values('best_test_acc', ascending=False)\n",
    ")\n",
    "print('Per-dataset summary:')\n",
    "display(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}